%% ----------------------------------------------------------------
%% Conclusions.tex
%% ---------------------------------------------------------------- 
\chapter{Conclusions} \label{Chapter: Conclusions}

%Introduction and Conclusion shouldn't include anything that it's not in the other parts.
%Objectives should match with conclusions.

%Conclusions mirrors the content of introduction and repeats/summarises the conclusion of the discussions. Nothing new here.

%Check that you end the paper with something worth remembering. This means something concrete. “More research is needed” is a platitude and a vague one at that; better, go for something like “because of the results of this paper, we are now in a position to tackle problem X with method Y, bringing us closer to the ultimate goal of Z”. This is far more concrete and memorable. Endings have power; do not waste this power.

%FUTURE WORK:
% CHeck saliency results when the saliencies for each class are only computed for the positions where that class wins
% TRain a network with a slightly wider window
% CHeck contribution of each layer by computing pre-dense saliency maps
% TRain network without one-hot amino-acids to prove that they are not that relevant
% APplying DeepLift
% DAta augmentation: due to the irrelevance of forward-backwards in the sequences it would be better to have a double database for purely CNNs approaches, with sequences trained in both ways

%\begin{enumerate}
%\item Let me summarise my conclusions about it
%\item And let me state the value of them
%\end{enumerate}

The field of biomedical research requires more than high accuracy; the ability to interpret models is also a valuable asset. Deep learning has increased the first point but fails at the second. Luckily, some interpretability techniques are being developed in the context of image processing and can be applied to other fields. This work has implemented for the first time saliency maps in a structure-to-structure sequential problem, namely, the protein secondary structure prediction problem. While saliency maps in problems with a single output per sequence are simple to interpret, structure-to-structure problems have more dimensions, so further methods for aggregating and inspecting them are also necessary. The aggregating methods shown here focus on sequences, inputs or outputs, uncovering different relations in the data. This type of analysis can be readily understood by experts in the field and provide them with valuable new information.

Preliminary inspection of the saliency map has reassured the superiority of \textit{pssm} scores as inputs, later confirmed by an empirical experiment. Other relevant facts include the skewness of $\alpha$-helices, which are influenced more strongly by the amino-acids coming afterwards. This asymmetry is also present in other classes and it varies depending on the \textit{pssm} we look at. From a class-level perspective, $\alpha$-helices have higher tendency to look at distant amino-acids (inside the limited window of 19); $\beta$-strands have more consideration for further positions as compared to other classes, but they still mainly focus on the close vicinity, so longer interactions does not seem to be captured by the network.

Still, further exploration techniques should be investigated. Other methods of aggregation can be explored since the ones here exposed only show features on a very general level. Clustering of the saliency maps would cover this aspect by including various facets of the features, and all techniques developed here can be applied independently to each of them and thus capture multi-modality. Feature visualisation techniques can also provide with extra valuable information, and their implementation should also be studied.

Finally, other kinds of problems can also be explored, such as backbone angle prediction, where the goal shifts from classification to regression.