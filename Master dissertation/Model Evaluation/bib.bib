@article{Zhou2015,
	abstract = {Identifying functional effects of noncoding variants is a major challenge in human genetics. To predict the noncoding-variant effects de novo from sequence, we developed a deep learning-based algorithmic framework, DeepSEA (http://deepsea.princeton.edu/), that directly learns a regulatory sequence code from large-scale chromatin-profiling data, enabling prediction of chromatin effects of sequence alterations with single-nucleotide sensitivity. We further used this capability to improve prioritization of functional variants including expression quantitative trait loci (eQTLs) and disease-associated variants.},
	archivePrefix = {arXiv},
	arxivId = {15334406},
	author = {Zhou, Jian and Troyanskaya, Olga G.},
	doi = {10.1038/nmeth.3547},
	eprint = {15334406},
	isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
	issn = {15487105},
	journal = {Nature Methods},
	keywords = {Computational Biology},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Computational Biology},
	number = {10},
	pages = {931--934},
	pmid = {26301843},
	title = {{Predicting effects of noncoding variants with deep learning-based sequence model}},
	volume = {12},
	year = {2015}
}


@article{Shrikumar2017,
	abstract = {The purported "black box"' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
	archivePrefix = {arXiv},
	arxivId = {1704.02685},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	eprint = {1704.02685},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrikumar, Greenside, Kundaje - 2017 - Learning Important Features Through Propagating Activation Differences.pdf:pdf},
	keywords = {Feature visualization,Saliency maps},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Saliency maps,Feature visualization},
	month = {apr},
	title = {{Learning Important Features Through Propagating Activation Differences}},
	url = {http://arxiv.org/abs/1704.02685},
	year = {2017}
}


@book{Ching2017,
	abstract = {Deep learning, which describes a class of machine learning algorithms, has recently showed impressive results across a variety of domains. Biology and medicine are data rich, but the data are complex and often ill-understood. Problems of this nature may be particularly well-suited to deep learning techniques. We examine applications of deep learning to a variety of biomedical problems -- patient classification, fundamental biological processes, and treatment of patients -- to predict whether deep learning will transform these tasks or if the biomedical sphere poses unique challenges. We find that deep learning has yet to revolutionize or definitively resolve any of these problems, but promising advances have been made on the prior state of the art. Even when improvement over a previous baseline has been modest, we have seen signs that deep learning methods may speed or aid human investigation. More work is needed to address concerns related to interpretability and how to best model each problem. Furthermore, the limited amount of labeled data for training presents problems in some domains, as can legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning powering changes at the bench and bedside with the potential to transform several areas of biology and medicine.},
	archivePrefix = {arXiv},
	arxivId = {142760},
	author = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Harris, David J. and DeCaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H. S. and Gitter, Anthony and Greene, Casey S.},
	booktitle = {bioRxiv},
	doi = {10.1101/142760},
	eprint = {142760},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ching et al. - 2017 - Opportunities And Obstacles For Deep Learning In Biology And Medicine.pdf:pdf},
	isbn = {0000000305396},
	keywords = {Broad},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Broad},
	title = {{Opportunities And Obstacles For Deep Learning In Biology And Medicine}},
	year = {2017}
}


@article{Montavon2017,
	abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
	archivePrefix = {arXiv},
	arxivId = {1512.02479},
	author = {Montavon, Gr{\'{e}}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"{u}}ller, Klaus Robert},
	doi = {10.1016/j.patcog.2016.11.008},
	eprint = {1512.02479},
	file = {:home/juillermo/Desktop/1512.02479v1.pdf:pdf},
	isbn = {0031-3203},
	issn = {00313203},
	journal = {Pattern Recognition},
	keywords = {Deep neural networks,Feature visualization,Heatmapping,Image recognition,Relevance propagation,Taylor decomposition},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	title = {{Explaining nonlinear classification decisions with deep Taylor decomposition}},
	year = {2017}
}


@misc{Montavon2018,
	abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.},
	annote = {Fucking awesome},
	archivePrefix = {arXiv},
	arxivId = {1706.07979},
	author = {Montavon, Gr{\'{e}}goire and Samek, Wojciech and M{\"{u}}ller, Klaus Robert},
	booktitle = {Digital Signal Processing: A Review Journal},
	doi = {10.1016/j.dsp.2017.10.011},
	eprint = {1706.07979},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Montavon, Samek, M{\"{u}}ller - 2018 - Methods for interpreting and understanding deep neural networks(2).pdf:pdf},
	issn = {10512004},
	keywords = {Activation maximization,Deep neural networks,Feature visualization,Layer-wise relevance propagation,Sensitivity analysis,Taylor decomposition},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	title = {{Methods for interpreting and understanding deep neural networks}},
	year = {2018}
}

@article{Lanchantin2016,
	abstract = {Deep neural network (DNN) models have recently obtained state-of-the-art prediction accuracy for the transcription factor binding (TFBS) site classification task. However, it remains unclear how these approaches identify meaningful DNA sequence signals and give insights as to why TFs bind to certain locations. In this paper, we propose a toolkit called the Deep Motif Dashboard (DeMo Dashboard) which provides a suite of visualization strategies to extract motifs, or sequence patterns from deep neural network models for TFBS classification. We demonstrate how to visualize and understand three important DNN models: convolutional, recurrent, and convolutional-recurrent networks. Our first visualization method is finding a test sequence's saliency map which uses first-order derivatives to describe the importance of each nucleotide in making the final prediction. Second, considering recurrent models make predictions in a temporal manner (from one end of a TFBS sequence to the other), we introduce temporal output scores, indicating the prediction score of a model over time for a sequential input. Lastly, a class-specific visualization strategy finds the optimal input sequence for a given TFBS positive class via stochastic gradient optimization. Our experimental results indicate that a convolutional-recurrent architecture performs the best among the three architectures. The visualization techniques indicate that CNN-RNN makes predictions by modeling both motifs as well as dependencies among them.},
	archivePrefix = {arXiv},
	arxivId = {1608.03644},
	author = {Lanchantin, Jack and Singh, Ritambhara and Wang, Beilun and Qi, Yanjun},
	doi = {10.1101/085191},
	eprint = {1608.03644},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lanchantin et al. - 2016 - Deep Motif Dashboard Visualizing and Understanding Genomic Sequences Using Deep Neural Networks.pdf:pdf;:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lanchantin et al. - 2016 - Deep Motif Dashboard Visualizing and Understanding Genomic Sequences Using Deep Neural Networks(2).pdf:pdf},
	issn = {23356936},
	keywords = {Deep learning,Feature visualization},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Deep learning,Feature visualization},
	title = {{Deep Motif Dashboard: Visualizing and Understanding Genomic Sequences Using Deep Neural Networks}},
	year = {2016}
}


@misc{chollet2015keras,
	title={Keras},
	author={Chollet, Fran\c{c}ois and others},
	year={2015},
	howpublished={\url{https://keras.io}},
}

@phdthesis{Fontal2017,
	author = {Fontal, Alejandro},
	file = {:home/juillermo/Desktop/edepotair{\_}t5a2fb523{\_}001.pdf:pdf},
	keywords = {CNN,Computational Biology,Deep Learning,Feature visualization,LSTM,Machine Learning,RNN,Subcellular localization},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {CNN,Computational Biology,Deep Learning,LSTM,Machine Learning,RNN,Subcellular localization,Feature visualization},
	school = {Wageningen University {\&} Research},
	title = {{Neural Networks for Subcellular Localization Prediction}},
	url = {http://edepot.wur.nl/429151},
	year = {2017}
}


@inproceedings{Zeiler2014,
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	archivePrefix = {arXiv},
	arxivId = {1311.2901},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	doi = {10.1007/978-3-319-10590-1_53},
	eprint = {1311.2901},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Fergus - 2014 - Visualizing and understanding convolutional networks.pdf:pdf},
	isbn = {9783319105895},
	issn = {16113349},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	pmid = {26353135},
	title = {{Visualizing and understanding convolutional networks}},
	year = {2014}
}


@article{Szegedy2013,
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	archivePrefix = {arXiv},
	arxivId = {1312.6199},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	eprint = {1312.6199},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:pdf},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	month = {dec},
	title = {{Intriguing properties of neural networks}},
	url = {http://arxiv.org/abs/1312.6199},
	year = {2013}
}


@article{Gatys2016,
	abstract = {Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic in-formation and, thus, allow to separate image content from style. Here we use image representations derived from Con-volutional Neural Networks optimised for object recogni-tion, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can sep-arate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an ar-bitrary photograph with the appearance of numerous well-known artworks. Our results provide new insights into the deep image representations learned by Convolutional Neu-ral Networks and demonstrate their potential for high level image synthesis and manipulation.},
	archivePrefix = {arXiv},
	arxivId = {1505.07376},
	author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
	doi = {10.1109/CVPR.2016.265},
	eprint = {1505.07376},
	file = {:home/juillermo/Desktop/07780634.pdf:pdf},
	isbn = {9781467388511},
	issn = {10636919},
	journal = {The IEEE conference on computer vision and pattern recognition},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	pmid = {15430064963552939126},
	title = {{Image style transfer using convolutional neural networks}},
	year = {2016}
}


@inproceedings{Mahendran2015,
	abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
	archivePrefix = {arXiv},
	arxivId = {1412.0035},
	author = {Mahendran, Aravindh and Vedaldi, Andrea},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	doi = {10.1109/CVPR.2015.7299155},
	eprint = {1412.0035},
	file = {:home/juillermo/Desktop/1412.0035v1.pdf:pdf},
	isbn = {9781467369640},
	issn = {10636919},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	pmid = {903},
	title = {{Understanding deep image representations by inverting them}},
	year = {2015}
}

@article{Simonyan2014,
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	file = {:home/juillermo/Desktop/1312.6034v2.pdf:pdf},
	journal = {arXiv.org},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
	year = {2014}
}

@misc{Mordvintsev2015,
	abstract = {Artificial Neural Networks have spurred remarkable recent progress in image classification and speech recognition. But even though these are very useful tools based on well-known mathematical methods, we actually understand surprisingly little of why certain models work and others don't. So let's take a look at some simple techniques for peeking inside these networks. We train an artificial neural network by showing it millions of training examples and gradually adjusting the network parameters until it gives the classifications we want. The network typically consists of 10-30 stacked layers of artificial neurons. Each image is fed into the input layer, which then talks to the next layer, until eventually the “output” layer is reached. The network's “answer” comes from this final output layer. One of the challenges of neural networks is understanding what exactly goes on at each layer. We know that after training, each layer progressively extracts higher and higher-level features of the image, until the final layer essentially makes a decision on what the image shows. For example, Artificial Neural Networks have spurred remarkable recent progress in image classification and speech recognition. But even though these are very useful tools based on well-known mathematical methods, we actually understand surprisingly little of why certain models work and others don't. So let's take a look at some simple techniques for peeking inside these networks.},
	author = {Mordvintsev, Alexander and Tyka, Michael and Olah, Christopher},
	booktitle = {Google Research Blog},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	title = {{Inceptionism: Going deeper into neural networks, google research blog}},
	url = {https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html},
	year = {2015}
}

@article{Olah2017,
	abstract = {Deep convolutional neural networks (CNNs) have demonstrated impressive performance on visual object classification tasks. In addition, it is a useful model for predication of neuronal responses recorded in visual system. However, there is still no clear understanding of what CNNs learn in terms of visual neuronal circuits. Visualizing CNN's features to obtain possible connections to neuronscience underpinnings is not easy due to highly complex circuits from the retina to higher visual cortex. Here we address this issue by focusing on single retinal ganglion cells with a simple model and electrophysiological recordings from salamanders. By training CNNs with white noise images to predicate neural responses, we found that convolutional filters learned in the end are resembling to biological components of the retinal circuit. Features represented by these filters tile the space of conventional receptive field of retinal ganglion cells. These results suggest that CNN could be used to reveal structure components of neuronal circuits.},
	archivePrefix = {arXiv},
	arxivId = {1711.02837},
	author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
	doi = {10.23915/distill.00007},
	eprint = {1711.02837},
	issn = {2476-0757},
	journal = {Distill},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	title = {{Feature Visualization}},
	url = {https://distill.pub/2017/feature-visualization/},
	year = {2017}
}

@article{Erhan2009,
	abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work},
	author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	file = {:home/juillermo/Desktop/visualization{\_}techreport.pdf:pdf},
	journal = {Bernoulli},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	title = {{Visualizing higher-layer features of a deep network}},
	year = {2009}
}
