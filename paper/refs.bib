%% ----------------------------------------------------------------
%% ECS.bib
%% ---------------------------------------------------------------- 
@article{He2015,
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\{}{\%}{\}} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\{}{\%}{\}} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\{}{\&}{\}} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
	archivePrefix = {arXiv},
	arxivId = {1512.03385},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	doi = {10.3389/fpsyg.2013.00124},
	eprint = {1512.03385},
	isbn = {978-1-4673-6964-0},
	issn = {1664-1078},
	journal = {arXiv preprint arXiv:1512.03385v1},
	keywords = {CNN,deep learning,denoising auto-encoder,image denoising},
	mendeley-groups = {AML},
	mendeley-tags = {CNN},
	pmid = {23554596},
	title = {{ResNet}},
	year = {2015}
}

@article{Shrikumar2016,
	abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Learning Important FeaTures), an efficient and effective method for computing importance scores in a neural network. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. We apply DeepLIFT to models trained on natural images and genomic data, and show significant advantages over gradient-based methods.},
	archivePrefix = {arXiv},
	arxivId = {1605.01713},
	author = {Shrikumar, A. and Greenside, P. and Shcherbina, A. and Kundaje, A.},
	eprint = {1605.01713},
	issn = {1938-7228},
	journal = {arXiv:1605.01713},
	keywords = {Saliency maps},
	mendeley-groups = {AML},
	mendeley-tags = {Saliency maps},
	title = {{Not Just a Black Box: Learning Important Features Through Propagating Activation Differences}},
	year = {2016}
}

@article{TheTheanoDevelopmentTeam2016,
	abstract = {Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.},
	archivePrefix = {arXiv},
	arxivId = {1605.02688},
	author = {{The Theano Development Team} and Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Fr{\'{e}}d{\'{e}}ric and Bayer, Justin and Belikov, Anatoly and Belopolsky, Alexander and Bengio, Yoshua and Bergeron, Arnaud and Bergstra, James and Bisson, Valentin and Snyder, Josh Bleecher and Bouchard, Nicolas and Boulanger-Lewandowski, Nicolas and Bouthillier, Xavier and de Br{\'{e}}bisson, Alexandre and Breuleux, Olivier and Carrier, Pierre-Luc and Cho, Kyunghyun and Chorowski, Jan and Christiano, Paul and Cooijmans, Tim and C{\^{o}}t{\'{e}}, Marc-Alexandre and C{\^{o}}t{\'{e}}, Myriam and Courville, Aaron and Dauphin, Yann N. and Delalleau, Olivier and Demouth, Julien and Desjardins, Guillaume and Dieleman, Sander and Dinh, Laurent and Ducoffe, M{\'{e}}lanie and Dumoulin, Vincent and Kahou, Samira Ebrahimi and Erhan, Dumitru and Fan, Ziye and Firat, Orhan and Germain, Mathieu and Glorot, Xavier and Goodfellow, Ian and Graham, Matt and Gulcehre, Caglar and Hamel, Philippe and Harlouchet, Iban and Heng, Jean-Philippe and Hidasi, Bal{\'{a}}zs and Honari, Sina and Jain, Arjun and Jean, S{\'{e}}bastien and Jia, Kai and Korobov, Mikhail and Kulkarni, Vivek and Lamb, Alex and Lamblin, Pascal and Larsen, Eric and Laurent, C{\'{e}}sar and Lee, Sean and Lefrancois, Simon and Lemieux, Simon and L{\'{e}}onard, Nicholas and Lin, Zhouhan and Livezey, Jesse A. and Lorenz, Cory and Lowin, Jeremiah and Ma, Qianli and Manzagol, Pierre-Antoine and Mastropietro, Olivier and McGibbon, Robert T. and Memisevic, Roland and van Merri{\"{e}}nboer, Bart and Michalski, Vincent and Mirza, Mehdi and Orlandi, Alberto and Pal, Christopher and Pascanu, Razvan and Pezeshki, Mohammad and Raffel, Colin and Renshaw, Daniel and Rocklin, Matthew and Romero, Adriana and Roth, Markus and Sadowski, Peter and Salvatier, John and Savard, Fran{\c{c}}ois and Schl{\"{u}}ter, Jan and Schulman, John and Schwartz, Gabriel and Serban, Iulian Vlad and Serdyuk, Dmitriy and Shabanian, Samira and Simon, {\'{E}}tienne and Spieckermann, Sigurd and Subramanyam, S. Ramana and Sygnowski, Jakub and Tanguay, J{\'{e}}r{\'{e}}mie and van Tulder, Gijs and Turian, Joseph and Urban, Sebastian and Vincent, Pascal and Visin, Francesco and de Vries, Harm and Warde-Farley, David and Webb, Dustin J. and Willson, Matthew and Xu, Kelvin and Xue, Lijun and Yao, Li and Zhang, Saizheng and Zhang, Ying},
	doi = {1605.02688},
	eprint = {1605.02688},
	journal = {arXiv e-prints},
	keywords = {Framework},
	mendeley-groups = {AML},
	mendeley-tags = {Framework},
	month = {may},
	title = {{Theano: A Python framework for fast computation of mathematical expressions}},
	url = {http://arxiv.org/abs/1605.02688},
	year = {2016}
}

@book{Goodfellow-et-al-2016,
	title={Deep Learning},
	author={I. Goodfellow and Y. Bengio and A. Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016}
}

@article{Srivastava2014,
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	archivePrefix = {arXiv},
	arxivId = {1102.4807},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	doi = {10.1214/12-AOS1000},
	eprint = {1102.4807},
	isbn = {1532-4435},
	issn = {15337928},
	journal = {Journal of Machine Learning Research},
	keywords = {Machine Learning,deep learning,model combination,neural networks,regularization},
	mendeley-groups = {AML},
	mendeley-tags = {Machine Learning},
	pmid = {23285570},
	title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
	year = {2014}
}
@article{SergeyIoffe2015,
	abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {{Sergey Ioffe}, Google and {Christian Szegedy}, Google},
	doi = {10.1007/s13398-014-0173-7.2},
	eprint = {arXiv:1011.1669v3},
	isbn = {9780874216561},
	issn = {10282092},
	journal = {Icml},
	keywords = {Blechnaceae,Indonesia,Machine Learning,New species,Stenochlaena},
	mendeley-groups = {AML},
	mendeley-tags = {Machine Learning},
	pmid = {15003161},
	title = {{Batch Normalization}},
	year = {2015}
}

@article{Dumoulin2016,
	archivePrefix = {arXiv},
	arxivId = {1603.07285},
	author = {Dumoulin, Vincent and Visin, Francesco},
	eprint = {1603.07285},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dumoulin, Visin - 2016 - A guide to convolution arithmetic for deep learning.pdf:pdf},
	keywords = {CNN},
	mendeley-groups = {AML},
	mendeley-tags = {CNN},
	month = {mar},
	title = {{A guide to convolution arithmetic for deep learning}},
	url = {http://arxiv.org/abs/1603.07285},
	year = {2016}
}

@article{Gu2017,
	abstract = {In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.},
	archivePrefix = {arXiv},
	arxivId = {1512.07108},
	author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Li and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
	doi = {10.3389/fpsyg.2013.00124},
	eprint = {1512.07108},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gu et al. - 2017 - Recent Advances in Convolutional Neural Networks.pdf:pdf},
	isbn = {1664-1078},
	issn = {16641078},
	journal = {Arxiv},
	keywords = {CNN,convolutional neural network,deep learning},
	mendeley-groups = {AML},
	mendeley-tags = {CNN},
	pmid = {23554596},
	title = {{Recent Advances in Convolutional Neural Networks}},
	year = {2017}
}

@article{LeCun1998,
	abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
	archivePrefix = {arXiv},
	arxivId = {1102.0183},
	author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
	doi = {10.1109/5.726791},
	eprint = {1102.0183},
	isbn = {0018-9219},
	issn = {00189219},
	journal = {Proceedings of the IEEE},
	keywords = {CNN,Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
	mendeley-groups = {AML},
	mendeley-tags = {CNN},
	pmid = {15823584},
	title = {{Gradient-based learning applied to document recognition}},
	year = {1998}
}

@article{Nguyen2016,
	abstract = {We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.},
	archivePrefix = {arXiv},
	arxivId = {1602.03616},
	author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
	eprint = {1602.03616},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Yosinski, Clune - 2016 - Multifaceted Feature Visualization Uncovering the Different Types of Features Learned By Each Neuron in.pdf:pdf},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	month = {feb},
	title = {{Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks}},
	url = {http://arxiv.org/abs/1602.03616},
	year = {2016}
}

@article{Hosseini-Asl2018,
	abstract = {Early diagnosis is playing an important role in preventing progress of the Alzheimer's disease (AD). This paper proposes to improve the prediction of AD with a deep 3D Convolutional Neural Network (3D-CNN), which can show generic features capturing AD biomarkers extracted from brain images, adapt to different domain datasets, and accurately classify subjects with improved fine-tuning method. The 3D-CNN is built upon a convolutional autoencoder, which is pre-trained to capture anatomical shape variations in structural brain MRI scans for source domain. Fully connected upper layers of the 3D-CNN are then fine-tuned for each task-specific AD classification in target domain. In this paper, deep supervision algorithm is used to improve the performance of already proposed 3D Adaptive CNN. Experiments on the ADNI MRI dataset without skull-stripping preprocessing have shown that the proposed 3D Deeply Supervised Adaptable CNN outperforms several proposed approaches, including 3D-CNN model, other CNN-based methods and conventional classifiers by accuracy and robustness. Abilities of the proposed network to generalize the features learnt and adapt to other domains have been validated on the CADDementia dataset.},
	author = {Hosseini-Asl, E and Ghazal, M and Mahmoud, A and Aslantas, A and Shalaby, A M and Casanova, M F and Barnes, G N and Gimel'farb, G and Keynton, R and El-Baz, A},
	doi = {10.2741/4606},
	issn = {1093-4715},
	journal = {Front Biosci (Landmark Ed)},
	keywords = {Diagnosis},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Diagnosis},
	pmid = {28930562},
	title = {{Alzheimer's disease diagnostics by a 3D deeply supervised adaptable convolutional network}},
	year = {2018}
}

@article{Cheng2016,
	abstract = {This paper performs a comprehensive study on the deep-learning-based computer-aided diagnosis (CADx) for the differential diagnosis of benign and malignant nodules/lesions by avoiding the potential errors caused by inaccurate image processing results (e.g., boundary segmentation), as well as the classification bias resulting from a less robust feature set, as involved in most conventional CADx algorithms. Specifically, the stacked denoising auto-encoder (SDAE) is exploited on the two CADx applications for the differentiation of breast ultrasound lesions and lung CT nodules. The SDAE architecture is well equipped with the automatic feature exploration mechanism and noise tolerance advantage, and hence may be suitable to deal with the intrinsically noisy property of medical image data from various imaging modalities. To show the outperformance of SDAE-based CADx over the conventional scheme, two latest conventional CADx algorithms are implemented for comparison. 10 times of 10-fold cross-validations are conducted to illustrate the efficacy of the SDAE-based CADx algorithm. The experimental results show the significant performance boost by the SDAE-based CADx algorithm over the two conventional methods, suggesting that deep learning techniques can potentially change the design paradigm of the CADx systems without the need of explicit design and selection of problem-oriented features.},
	author = {Cheng, Jie-Zhi and Ni, Dong and Chou, Yi-Hong and Qin, Jing and Tiu, Chui-Mei and Chang, Yeun-Chung and Huang, Chiun-Sheng and Shen, Dinggang and Chen, Chung-Ming},
	doi = {10.1038/srep24454},
	isbn = {2045-2322},
	issn = {2045-2322},
	journal = {Scientific Reports},
	keywords = {Diagnosis},
	mendeley-groups = {AML},
	mendeley-tags = {Diagnosis},
	pmid = {27079888},
	title = {{Computer-Aided Diagnosis with Deep Learning Architecture: Applications to Breast Lesions in US Images and Pulmonary Nodules in CT Scans}},
	year = {2016}
}

@misc{Mahmud2018,
	abstract = {Rapid advances of hardware-based technologies during the past decades have opened up new possibilities for Life scientists to gather multimodal data in various application domains (e.g., Omics, Bioimaging, Medical Imaging, and [Brain/Body]-Machine Interfaces), thus generating novel opportunities for development of dedicated data intensive machine learning techniques. Overall, recent research in Deep learning (DL), Reinforcement learning (RL), and their combination (Deep RL) promise to revolutionize Artificial Intelligence. The growth in computational power accompanied by faster and increased data storage and declining computing costs have already allowed scientists in various fields to apply these techniques on datasets that were previously intractable for their size and complexity. This review article provides a comprehensive survey on the application of DL, RL, and Deep RL techniques in mining Biological data. In addition, we compare performances of DL techniques when applied to different datasets across various application domains. Finally, we outline open issues in this challenging research area and discuss future development perspectives.},
	archivePrefix = {arXiv},
	arxivId = {1711.03985},
	author = {Mahmud, Mufti and Kaiser, Mohammed Shamim and Hussain, Amir and Vassanelli, Stefano},
	booktitle = {IEEE Transactions on Neural Networks and Learning Systems},
	doi = {10.1109/TNNLS.2018.2790388},
	eprint = {1711.03985},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahmud et al. - 2018 - Applications of Deep Learning and Reinforcement Learning to Biological Data.pdf:pdf},
	issn = {21622388},
	keywords = {Bioimaging,Broad,brain-machine interfaces,convolutional neural network (CNN),deep autoencoder (DA),deep belief network (DBN),deep learning performance,medical imaging,omics,recurrent neural network (RNN)},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Broad},
	pmid = {29771663},
	title = {{Applications of Deep Learning and Reinforcement Learning to Biological Data}},
	year = {2018}
}

@article{Jones2017,
	abstract = {Deep learning is the trendiest tool in a computational biologist's toolbox. This exciting class of methods, based on artificial neural networks, quickly became popular due to its competitive performance in prediction problems. In pioneering early work, applying simple network architectures to abundant data already provided gains over traditional counterparts in functional genomics, image analysis, and medical diagnostics. Now, ideas for constructing and training networks and even off-the-shelf models have been adapted from the rapidly developing machine learning subfield to improve performance in a range of computational biology tasks. Here, we review some of these advances in the last 2 years. * CT, : computer tomography; MRI, : magnetic resonance imaging},
	annote = {Awesome exhaustive table with plenty of state-of-the-art architectures, the problems to which they were applied and their specific applications.},
	author = {Jones, W. and Alasoo, K. and Fishman, D. and Parts, L.},
	doi = {10.1042/ETLS20160025},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jones et al. - 2017 - Computational biology deep learning.pdf:pdf},
	issn = {2397-8554},
	journal = {Emerging Topics in Life Sciences},
	keywords = {Broad,Computational Biology,Deep learning},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Broad,Computational Biology,Deep learning},
	number = {3},
	pages = {257--274},
	title = {{Computational biology: deep learning}},
	url = {http://www.emergtoplifesci.org/lookup/doi/10.1042/ETLS20160025},
	volume = {1},
	year = {2017}
}

@book{Ching2017,
	abstract = {Deep learning, which describes a class of machine learning algorithms, has recently showed impressive results across a variety of domains. Biology and medicine are data rich, but the data are complex and often ill-understood. Problems of this nature may be particularly well-suited to deep learning techniques. We examine applications of deep learning to a variety of biomedical problems -- patient classification, fundamental biological processes, and treatment of patients -- to predict whether deep learning will transform these tasks or if the biomedical sphere poses unique challenges. We find that deep learning has yet to revolutionize or definitively resolve any of these problems, but promising advances have been made on the prior state of the art. Even when improvement over a previous baseline has been modest, we have seen signs that deep learning methods may speed or aid human investigation. More work is needed to address concerns related to interpretability and how to best model each problem. Furthermore, the limited amount of labeled data for training presents problems in some domains, as can legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning powering changes at the bench and bedside with the potential to transform several areas of biology and medicine.},
	archivePrefix = {arXiv},
	arxivId = {142760},
	author = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Harris, David J. and DeCaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H. S. and Gitter, Anthony and Greene, Casey S.},
	booktitle = {bioRxiv},
	doi = {10.1101/142760},
	eprint = {142760},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ching et al. - 2017 - Opportunities And Obstacles For Deep Learning In Biology And Medicine.pdf:pdf},
	isbn = {0000000305396},
	keywords = {Broad},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Broad},
	title = {{Opportunities And Obstacles For Deep Learning In Biology And Medicine}},
	year = {2017}
}

@misc{Mamoshina2016,
	abstract = {Increases in throughput and installed base of biomedical research equipment led to a massive accumulation of -omics data known to be highly variable, high-dimensional, and sourced from multiple often incompatible data platforms. While this data may be useful for biomarker identification and drug discovery, the bulk of it remains underutilized. Deep neural networks (DNNs) are efficient algorithms based on the use of compositional layers of neurons, with advantages well matched to the challenges -omics data presents. While achieving state-of-the-art results and even surpassing human accuracy in many challenging tasks, the adoption of deep learning in biomedicine has been comparatively slow. Here, we discuss key features of deep learning that may give this approach an edge over other machine learning methods. We then consider limitations and review a number of applications of deep learning in biomedical studies demonstrating proof of concept and practical utility.},
	annote = {Good explanation of the different biological problems being addressed by ML (section 5)
	
	Awesome figure showing all possible sources of data},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1312.6184v5},
	author = {Mamoshina, Polina and Vieira, Armando and Putin, Evgeny and Zhavoronkov, Alex},
	booktitle = {Molecular Pharmaceutics},
	doi = {10.1021/acs.molpharmaceut.5b00982},
	eprint = {arXiv:1312.6184v5},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mamoshina et al. - 2016 - Applications of Deep Learning in Biomedicine.pdf:pdf},
	isbn = {1543-8392 (Electronic)$\backslash$r1543-8384 (Linking)},
	issn = {15438392},
	keywords = {Broad,RBM,artificial intelligence,biomarker development,deep learning,deep neural networks,genomics,transcriptomics},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Broad},
	number = {5},
	pages = {1445--1454},
	pmid = {27007977},
	title = {{Applications of Deep Learning in Biomedicine}},
	volume = {13},
	year = {2016}
}

@article{Baehrens2009,
	abstract = {After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted the particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.},
	archivePrefix = {arXiv},
	arxivId = {0912.1128},
	author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and Mueller, Klaus-Robert},
	eprint = {0912.1128},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baehrens et al. - 2009 - How to Explain Individual Classification Decisions.pdf:pdf},
	keywords = {Saliency maps},
	mendeley-groups = {AML},
	mendeley-tags = {Saliency maps},
	month = {dec},
	title = {{How to Explain Individual Classification Decisions}},
	url = {http://arxiv.org/abs/0912.1128},
	year = {2009}
}

@article{Hansen2011,
	abstract = {Statistical models are frequently used to estimate molecular properties, e.g., to establish quantitative structure-activity and structure-property relationships. For such models, interpretability, knowledge of the domain of applicability, and an estimate of confidence in the predictions are essential. We develop and validate a method for the interpretation of kernel-based prediction models. As a consequence of interpretability, the method helps to assess the domain of applicability of a model, to judge the reliability of a prediction, and to determine relevant molecular features. Increased interpretability also facilitates the acceptance of such models. Our method is based on visualization: For each prediction, the most contributing training samples are computed and visualized. We quantitatively show the effectiveness of our approach by conducting a questionnaire study with 71 participants, resulting in significant improvements of the participants' ability to distinguish between correct and incorrect predictions of a Gaussian process model for Ames mutagenicity. {\textcopyright} 2011 Wiley-VCH Verlag GmbH {\&} Co. KGaA, Weinheim.},
	author = {Hansen, Katja and Baehrens, David and Schroeter, Timon and Rupp, Matthias and M{\"{u}}ller, Klaus Robert},
	doi = {10.1002/minf.201100059},
	issn = {18681743},
	journal = {Molecular Informatics},
	keywords = {Confidence estimation,Domain of applicability,Kernel-based learning,QSAR,QSPR,Saliency maps},
	mendeley-groups = {AML},
	mendeley-tags = {Saliency maps},
	title = {{Visual interpretation of kernel-based prediction models}},
	year = {2011}
}

@article{Springenberg2014,
	abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
	archivePrefix = {arXiv},
	arxivId = {1412.6806},
	author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	eprint = {1412.6806},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Springenberg et al. - 2014 - Striving for Simplicity The All Convolutional Net.pdf:pdf},
	keywords = {Saliency maps},
	mendeley-tags = {Saliency maps},
	month = {dec},
	title = {{Striving for Simplicity: The All Convolutional Net}},
	url = {http://arxiv.org/abs/1412.6806},
	year = {2014}
}

@article{Bach2015,
	abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	archivePrefix = {arXiv},
	arxivId = {1606.04155},
	author = {Bach, S. and Binder, A. and Montavon, G. and Klauschen, F. and M{\"{u}}ller, K. R. and Samek, W.},
	doi = {10.1371/journal.pone.0130140},
	eprint = {1606.04155},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2015 - On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.pdf:pdf},
	isbn = {10.1371/journal.pone.0130140},
	issn = {19326203},
	journal = {PLoS ONE 10(7): e0130140},
	keywords = {Saliency maps},
	mendeley-groups = {AML},
	mendeley-tags = {Saliency maps},
	pmid = {26161953},
	title = {{On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation}},
	year = {2015}
}

@misc{Min2017,
	abstract = {As we are living in the era of big data, transforming biomedical big data into valuable knowledge has been one of the most important problems in bioinformatics. At the same time, deep learning has advanced rapidly since early 2000s and is recently showing a state-of-the-art performance in various fields. So naturally, applying deep learning in bioinformatics to gain insights from data is under the spotlight of both the academia and the industry. This article reviews some research of deep learning in bioinformatics. To provide a big picture, we categorized the research by both bioinformatics domains (i.e., omics, biomedical imaging, biomedical signal processing) and deep learning architectures (i.e., deep neural network, convolutional neural network, recurrent neural network, modified neural network) as well as present brief descriptions of each work. Additionally, we introduce a few issues of deep learning in bioinformatics such as problems of class imbalance data and suggest future research directions such as multimodal deep learning. We believe that this paper could provide valuable insights and be a starting point for researchers to apply deep learning in their bioinformatics studies.},
	annote = {GOOD PAPER
	
	Good table about the different broad architectures and the main research topics in Bioinformatics},
	archivePrefix = {arXiv},
	arxivId = {1603.06430},
	author = {Min, Seonwoo and Lee, Byunghan and Yoon, Sungroh},
	booktitle = {Briefings in bioinformatics},
	doi = {10.1093/bib/bbw068},
	eprint = {1603.06430},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Min, Lee, Yoon - 2017 - Deep learning in bioinformatics.pdf:pdf},
	isbn = {978-1-4614-6827-1, 978-1-4614-6828-8},
	issn = {14774054},
	keywords = {Broad,CNN,Computational Biology,Deep learning,LSTM,bioinformatics,biomedical imaging,biomedical signal processing,deep learning,machine learning,neural network,omics},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Broad,CNN,Computational Biology,Deep learning,LSTM},
	number = {5},
	pages = {851--869},
	pmid = {27473064},
	title = {{Deep learning in bioinformatics}},
	volume = {18},
	year = {2017}
}

@article{Moeskops2016,
	abstract = {—Automatic segmentation in MR brain images is im-portant for quantitative analysis in large-scale studies with images acquired at all ages. This paper presents a method for the auto-matic segmentation of MR brain images into a number of tissue classes using a convolutional neural network. To ensure that the method obtains accurate segmentation details as well as spatial consistency, the network uses multiple patch sizes and multiple convolution kernel sizes to acquire multi-scale information about each voxel. The method is not dependent on explicit features, but learns to recognise the information that is important for the clas-sification based on training data. The method requires a single anatomical MR image only. The segmentation method is applied to five different data sets: coronal -weighted images of preterm in-fants acquired at 30 weeks postmenstrual age (PMA) and 40 weeks PMA, axial -weighted images of preterm infants acquired at 40 weeks PMA, axial -weighted images of ageing adults acquired at an average age of 70 years, and -weighted images of young adults acquired at an average age of 23 years. The method ob-tained the following average Dice coefficients over all segmented tissue classes for each data set, respectively: 0.87, 0.82, 0.84, 0.86, and 0.91. The results demonstrate that the method obtains accu-rate segmentations in all five sets, and hence demonstrates its ro-bustness to differences in age and acquisition protocol. Index Terms—Adult brain, automatic image segmentation, con-volutional neural networks, deep learning, MRI, preterm neonatal brain.},
	archivePrefix = {arXiv},
	arxivId = {1704.03295},
	author = {Moeskops, Pim and Viergever, Max A. and Mendrik, Adrienne M. and {De Vries}, Linda S. and Benders, Manon J.N.L. and Isgum, Ivana},
	doi = {10.1109/TMI.2016.2548501},
	eprint = {1704.03295},
	isbn = {0278-0062},
	issn = {1558254X},
	journal = {IEEE Transactions on Medical Imaging},
	keywords = {Adult brain,Image segmentation,MRI,automatic image segmentation,convolutional neural networks,deep learning,preterm neonatal brain},
	mendeley-groups = {AML},
	mendeley-tags = {Image segmentation},
	pmid = {27046893},
	title = {{Automatic Segmentation of MR Brain Images with a Convolutional Neural Network}},
	year = {2016}
}

@article{Cha2016,
	abstract = {Purpose: The authors are developing a computerized system for bladder segmentation in CT urography (CTU) as a critical component for computer-aided detection of bladder cancer. Methods: A deep-learning convolutional neural network (DL-CNN) was trained to distinguish between the inside and the outside of the bladder using 160 000 regions of interest (ROI) from CTU images. The trained DL-CNN was used to estimate the likelihood of an ROI being inside the bladder for ROIs centered at each voxel in a CTU case, resulting in a likelihood map. Thresholding and hole-filling were applied to the map to generate the initial contour for the bladder, which was then refined by 3D and 2D level sets. The segmentation performance was evaluated using 173 cases: 81 cases in the training set (42 lesions, 21 wall thickenings, and 18 normal bladders) and 92 cases in the test set (43 lesions, 36 wall thickenings, and 13 normal bladders). The computerized segmentation accuracy using the DL likelihood map was compared to that using a likelihood map generated by Haar features and a random forest classifier, and that using our previous conjoint level set analysis and segmentation system (CLASS) without using a likelihood map. All methods were evaluated relative to the 3D hand-segmented reference contours. Results: With DL-CNN-based likelihood map and level sets, the average volume intersection ratio, average percent volume error, average absolute volume error, average minimum distance, and the Jaccard index for the test set were 81.9{\%}±12.1{\%}, 10.2{\%}±16.2{\%}, 14.0{\%}±13.0{\%}, 3.6±2.0 mm, and 76.2{\%}±11.8{\%}, respectively. With the Haar-feature-based likelihood map and level sets, the corresponding values were 74.3{\%}±12.7{\%}, 13.0{\%}±22.3{\%}, 20.5{\%}±15.7{\%}, 5.7±2.6 mm, and 66.7{\%}±12.6{\%}, respectively.With our previous CLASS with local contour refinement (LCR) method, the corresponding values were 78.0{\%}±14.7{\%}, 16.5{\%}±16.8{\%}, 18.2{\%}±15.0{\%}, 3.8±2.3 mm, and 73.9{\%}±13.5{\%}, respectively. Conclusions: The authors demonstrated that the DL-CNN can overcome the strong boundary between two regions that have large difference in gray levels and provides a seamless mask to guide level set segmentation, which has been a problem for many gradient-based segmentation methods. Compared to our previous CLASS with LCR method, which required two user inputs to initialize the segmentation, DL-CNN with level sets achieved better segmentation performance while using a single user input. Compared to the Haar-feature-based likelihood map, the DL-CNN-based likelihood map could guide the level sets to achieve better segmentation. The results demonstrate the feasibility of our new approach of using DL-CNN in combination with level sets for segmentation of the bladder.},
	author = {Cha, Kenny H. and Hadjiiski, Lubomir and Samala, Ravi K. and Chan, Heang-Ping and Caoili, Elaine M. and Cohan, Richard H.},
	doi = {10.1118/1.4944498},
	isbn = {9781510600201},
	issn = {00942405},
	journal = {Medical Physics},
	keywords = {Image segmentation},
	mendeley-groups = {AML},
	mendeley-tags = {Image segmentation},
	pmid = {27036584},
	title = {{Urinary bladder segmentation in CT urography using deep-learning convolutional neural network and level sets}},
	year = {2016}
}

@inproceedings{Lee2009,
	abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1301.3605v3},
	author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
	doi = {10.1145/1553374.1553453},
	eprint = {arXiv:1301.3605v3},
	isbn = {9781605585161},
	issn = {02643294},
	keywords = {CNN,Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {CNN,Feature visualization},
	pmid = {20957573},
	title = {{Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations}},
	year = {2009}
}

@misc{Dieleman2015,
	abstract = {core contributors, in alphabetical order: Eric Battenberg (@ebattenberg) Sander Dieleman (@benanne) Daniel Nouri (@dnouri) Eben Olson (@ebenolson) A{\"{a}}ron van den Oord (@avdnoord) Colin Raffel (@craffel) Jan Schl{\"{u}}ter (@f0k) S{\o}ren Kaae S{\o}nderby (@skaae) extra contributors, in chronological order: Daniel Maturana (@dimatura): documentation, cuDNN layers, LRN Jonas Degrave (@317070): get{\_}all{\_}param{\_}values() fix Jack Kelly (@JackKelly): help with recurrent layers G{\'{a}}bor Tak{\'{a}}cs (@takacsg84): support broadcastable parameters in lasagne.updates Diogo Moitinho de Almeida (@diogo149): MNIST example fixes Brian McFee (@bmcfee): MaxPool2DLayer fix Martin Thoma (@MartinThoma): documentation Jeffrey De Fauw (@JeffreyDF): documentation, ADAM fix Michael Heilman (@mheilman): NonlinearityLayer, lasagne.random Gregory Sanders (@instagibbs): documentation fix Jon Crall (@erotemic): check for non-positive input shapes Hendrik Weideman (@hjweide): set{\_}all{\_}param{\_}values() test, MaxPool2DCCLayer fix Kashif Rasul (@kashif): ADAM simplification Peter de Rivaz (@peterderivaz): documentation fix},
	author = {Dieleman, Sander and Schl{\"{u}}ter, Jan and Raffel, Colin and Olson, Eben and S{\o}nderby, S{\o}ren Kaae and Nouri, Daniel and Maturana, Daniel and Thoma, Martin and Battenberg, Eric and Kelly, Jack and Fauw, Jeffrey De and Heilman, Michael and Diogo149 and McFee, Brian and Weideman, Hendrik and Takacsg84 and Peterderivaz and Jon and Instagibbs and Rasul, Dr Kashif and CongLiu and Britefury and Degrave, Jonas},
	booktitle = {Zenodo},
	doi = {10.5281/zenodo.27878},
	keywords = {Framework},
	mendeley-groups = {AML},
	mendeley-tags = {Framework},
	title = {{Lasagne: First release.}},
	year = {2015}
}

@article{Thomsen2012,
	abstract = {Seq2Logo is a web-based sequence logo generator. Sequence logos are a graphical representation of the information content stored in a multiple sequence alignment (MSA) and provide a compact and highly intuitive representation of the position-specific amino acid composition of binding motifs, active sites, etc. in biological sequences. Accurate generation of sequence logos is often compromised by sequence redundancy and low number of observations. Moreover, most methods available for sequence logo generation focus on displaying the position-specific enrichment of amino acids, discarding the equally valuable information related to amino acid depletion. Seq2logo aims at resolving these issues allowing the user to include sequence weighting to correct for data redundancy, pseudo counts to correct for low number of observations and different logotype representations each capturing different aspects related to amino acid enrichment and depletion. Besides allowing input in the format of peptides and MSA, Seq2Logo accepts input as Blast sequence profiles, providing easy access for non-expert end-users to characterize and identify functionally conserved/variable amino acids in any given protein of interest. The output from the server is a sequence logo and a PSSM. Seq2Logo is available at http://www.cbs.dtu.dk/biotools/Seq2Logo (14 May 2012, date last accessed).},
	author = {Thomsen, M. C. F. and Nielsen, M.},
	doi = {10.1093/nar/gks469},
	isbn = {1362-4962 (Electronic) 0305-1048 (Linking)},
	issn = {03051048},
	journal = {Nucleic Acids Research},
	mendeley-groups = {Deep Biology},
	number = {W1},
	pmid = {22638583},
	title = {{Seq2Logo: A method for construction and visualization of amino acid binding motifs and sequence profiles including sequence weighting, pseudo counts and two-sided representation of amino acid enrichment and depletion}},
	volume = {40},
	year = {2012}
}

@article{Alipanahi2015,
	abstract = {Knowing the sequence specificities of DNA- and RNA-binding proteins is essential for developing models of the regulatory processes in biological systems and for identifying causal disease variants. Here we show that sequence specificities can be ascertained from experimental data with 'deep learning' techniques, which offer a scalable, flexible and unified computational approach for pattern discovery. Using a diverse array of experimental data and evaluation metrics, we find that deep learning outperforms other state-of-the-art methods, even when training on in vitro data and testing on in vivo data. We call this approach DeepBind and have built a stand-alone software tool that is fully automatic and handles millions of sequences per experiment. Specificities determined by DeepBind are readily visualized as a weighted ensemble of position weight matrices or as a 'mutation map' that indicates how variations affect binding within a specific sequence.},
	archivePrefix = {arXiv},
	arxivId = {cs/9605103},
	author = {Alipanahi, B. and Delong, A. and Weirauch, M. T. and Frey, B. J.},
	doi = {10.1038/nbt.3300},
	eprint = {9605103},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alipanahi et al. - 2015 - Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning.pdf:pdf},
	isbn = {1087-0156 1546-1696},
	issn = {15461696},
	journal = {Nature Biotechnology},
	keywords = {Computational Biology,DNA/RNA targets,Deep learning,Feature visualization},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Computational Biology,DNA/RNA targets,Deep learning,Feature visualization},
	number = {8},
	pages = {831--838},
	pmid = {26213851},
	primaryClass = {cs},
	title = {{Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning}},
	volume = {33},
	year = {2015}
}
@article{Quang2016,
	abstract = {Modeling the properties and functions of DNA sequences is an important, but challenging task in the broad field of genomics. This task is particularly difficult for non-coding DNA, the vast majority of which is still poorly understood in terms of function. A powerful predictive model for the function of non-coding DNA can have enormous benefit for both basic science and translational research because over 98{\%} of the human genome is non-coding and 93{\%} of disease-associated variants lie in these regions. To address this need, we propose DanQ, a novel hybrid convolutional and bi-directional long short-term memory recurrent neural network framework for predicting non-coding functionde novofrom sequence. In the DanQ model, the convolution layer captures regulatory motifs, while the recurrent layer captures long-term dependencies between the motifs in order to learn a regulatory 'grammar' to improve predictions. DanQ improves considerably upon other models across several metrics. For some regulatory markers, DanQ can achieve over a 50{\%} relative improvement in the area under the precision-recall curve metric compared to related models. We have made the source code available at the github repositoryhttp://github.com/uci-cbcl/DanQ.},
	annote = {Filter kernels as motifs.},
	author = {Quang, D. and Xie, X.},
	doi = {10.1093/nar/gkw226},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quang, Xie - 2016 - DanQ A hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences.pdf:pdf},
	isbn = {10.1101/032821},
	issn = {13624962},
	journal = {Nucleic Acids Research},
	keywords = {CNN},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {CNN},
	pmid = {27084946},
	title = {{DanQ: A hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences}},
	year = {2016}
}

@article{Umarov2017,
	abstract = {copyrighted software$\backslash$r$\backslash$nhttp://www.softberry.com/berry.phtml?topic=fdp.htm{\&}no{\_}menu=on$\backslash$r$\backslash$n$\backslash$r$\backslash$nAccurate computational identification of promoters remains a challenge as these key DNA regulatory regions have variable structures composed of functional motifs that provide gene-specific initiation of transcription. In this paper we utilize Convolutional Neural Networks (CNN) to analyze sequence characteristics of prokaryotic and eukaryotic promoters and build their predictive models. We trained a similar CNN architecture on promoters of five distant organisms: human, mouse, plant (Arabidopsis), and two bacteria (Escherichia coli and Bacillus subtilis). We found that CNN trained on sigma70 subclass of Escherichia coli promoter gives an excellent classification of promoters and non-promoter sequences (Sn = 0.90, Sp = 0.96, CC = 0.84). The Bacillus subtilis promoters identification CNN model achieves Sn = 0.91, Sp = 0.95, and CC = 0.86. For human, mouse and Arabidopsis promoters we employed CNNs for identification of two well-known promoter classes (TATA and non-TATA promoters). CNN models nicely recognize these complex functional regions. For human promoters Sn/Sp/CC accuracy of prediction reached 0.95/0.98/0,90 on TATA and 0.90/0.98/0.89 for non-TATA promoter sequences, respectively. For Arabidopsis we observed Sn/Sp/CC 0.95/0.97/0.91 (TATA) and 0.94/0.94/0.86 (non-TATA) promoters. Thus, the developed CNN models, implemented in CNNProm program, demonstrated the ability of deep learning approach to grasp complex promoter sequence characteristics and achieve significantly higher accuracy compared to the previously developed promoter prediction programs. We also propose random substitution procedure to discover positionally conserved promoter functional elements. As the suggested approach does not require knowledge of any specific promoter features, it can be easily extended to identify promoters and other complex functional regions in sequences of many other and especially newly sequenced genomes. The CNNProm program is available to run at web server http://www.softberry.com.$\backslash$r$\backslash$n$\backslash$r$\backslash$n},
	archivePrefix = {arXiv},
	arxivId = {1610.00121},
	author = {Umarov, R. K. and Solovyev, V. V.},
	doi = {10.1371/journal.pone.0171410},
	eprint = {1610.00121},
	isbn = {1111111111},
	issn = {19326203},
	journal = {PLoS ONE, 12(2): e0171410},
	mendeley-groups = {Deep Biology},
	pmid = {28158264},
	title = {{Recognition of prokaryotic and eukaryotic promoters using convolutional deep learning neural networks}},
	year = {2017}
}

@article{Kelley2016,
	abstract = {The complex language of eukaryotic gene expression remains incompletely understood. Thus, most of the many noncoding variants statistically associated with human disease have unknown mechanism. Here, we address this challenge using an approach based on a recent machine learning advance—deep convolutional neural networks (CNNs). We introduce an open source package Basset (https://github.com/davek44/Basset) to apply deep CNNs to learn the functional activity of DNA sequences from genomics data. We trained Basset on a compendium of accessible genomic sites mapped in 164 cell types by DNaseI-seq. Basset predictions for the change in accessibility between two variant alleles were far greater for GWAS SNPs that are likely to be causal relative to nearby SNPs in linkage disequilibrium with them. With Basset, a researcher can perform a single sequencing assay in their cell type of interest and simultaneously learn that cell???s chromatin accessibility code and annotate every mutation in the genome with its influence on present accessibility and latent potential for accessibility. Thus, Basset offers a powerful computational approach to annotate and interpret the noncoding genome.},
	archivePrefix = {arXiv},
	arxivId = {1408.5405},
	author = {Kelley, D. R. and Snoek, J. and Rinn, J. L.},
	doi = {10.1101/gr.200535.115},
	eprint = {1408.5405},
	isbn = {1367-4803},
	issn = {15495469},
	journal = {Genome Research},
	volume = {6},
	number = {7},
	pages = {990-999},
	mendeley-groups = {Deep Biology},
	pmid = {27197224},
	title = {{Basset: Learning the regulatory code of the accessible genome with deep convolutional neural networks}},
	year = {2016}
}

@article{Finnegan2017,
	abstract = {New architectures of multilayer artificial neural networks and new methods for training them are rapidly revolutionizing the application of machine learning in diverse fields, including business, social science, physical sciences, and biology. Interpreting deep neural networks, however, currently remains elusive, and a critical challenge lies in understanding which meaningful features a network is actually learning. We present a general method for inter-preting deep neural networks and extracting network-learned features from input data. We describe our algorithm in the context of biological sequence analysis. Our approach, based on ideas from statistical physics, samples from the maximum entropy distribution over possi-ble sequences, anchored at an input sequence and subject to constraints implied by the empirical function learned by a network. Using our framework, we demonstrate that local transcription factor binding motifs can be identified from a network trained on ChIP-seq data and that nucleosome positioning signals are indeed learned by a network trained on chemi-cal cleavage nucleosome maps. Imposing a further constraint on the maximum entropy dis-tribution also allows us to probe whether a network is learning global sequence features, such as the high GC content in nucleosome-rich regions. This work thus provides valuable mathematical tools for interpreting and extracting learned features from feed-forward neural networks. Author summary},
	author = {Finnegan, A. and Song, J. S.},
	doi = {10.1371/journal.pcbi.1005836},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Finnegan, Song - 2017 - Maximum entropy methods for extracting the learned features of deep neural networks.pdf:pdf},
	isbn = {1111111111},
	issn = {15537358},
	journal = {PLoS Comput Biol 13(10): e1005836},
	keywords = {Feature visualization,Saliency maps},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Feature visualization,Saliency maps},
	pmid = {29084280},
	title = {{Maximum entropy methods for extracting the learned features of deep neural networks}},
	year = {2017}
}

@article{Chou1974,
	abstract = {A new predictive model for the secondary structure of globular proteins (a helix, P sheet, and B turns) is de- scribed utilizing the helix and P-sheet conformational param- eters, P, and P p , of the 20 amino acids computed in the preceding paper (Chou and Fasman, 1974). This simple and direct method, devoid of complex computer calculations, utilizes empirical rules for predicting the initiation and termination of helical and regions in proteins. Briefly stated: when four helix formers out of six residues or three /? formers out of five residues are found clustered together in any native protein segment, the nucleation of these secondary structures begins and propagates in both directions until terminated by a se- quence of tetrapeptides, designated as breakers. These rules were successful in locating 8 z of helical and 95{\%} of B re- gions, as well as correctly predicting 80{\%} of the helical and 86{\%} of the @-sheetresidues in the 19 proteins evaluated. The accuracy of predicting the three conformational states for all residues, helix, P, and coil, is 7 7 z and shows great improve- ment over earlier prediction methods which considered only the helix and coil states. The p-turn conformational param- eters, Pt, for all 20 amino acids are computed. Their use en- ables the prediction of chain reversal and tertiary folding in proteins. A procedure for predicting conformational changes in specific regions is also outlined. Despite some evidence of long-range interactions in stabilizing protein folding, the pres- ent predictive model illustrates that short-range interactions (i.e., single residue information as represented by P , and Po) and medium-range interactions neighboring residue in- formation as represented by ( P a ) and (Pp)) play the pre- dominant role in determining protein secondary structure. Although the three-dimensional structures of only 19 proteins have been elucidated to date cia X-ray studies, the amino acid sequences of hundreds of proteins have already been deter- mined. Since the present predictive model is capable of de- lineating the helix, @, and coil regions of proteins of known sequence with 80{\%} accuracy, application of this method will be of assistance to all those interested in studying the correla- tion between protein conformation and biological activity as well as an aid to crystallographers in interpreting X-ray data.},
	author = {Chou, P. Y. and Fasman, G. D.},
	doi = {10.1021/bi00699a002},
	isbn = {0006-2960 (Print)$\backslash$r0006-2960 (Linking)},
	issn = {15204995},
	journal = {Biochemistry},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {4358940},
	title = {{Prediction of Protein Conformation}},
	year = {1974}
}

@misc{Rost2001,
	abstract = {Methods predicting protein secondary structure improved substantially in the 1990s through the use of evolutionary information taken from the divergence of proteins in the same structural family. Recently, the evolutionary information resulting from improved searches and larger databases has again boosted prediction accuracy by more than four percentage points to its current height of around 76{\%} of all residues predicted correctly in one of the three states, helix, strand, and other. The past year also brought successful new concepts to the field. These new methods may be particularly interesting in light of the improvements achieved through simple combining of existing methods. Divergent evolutionary profiles contain enough information not only to substantially improve prediction accuracy, but also to correctly predict long stretches of identical residues observed in alternative secondary structure states depending on nonlocal conditions. An example is a method automatically identifying structural switches and thus finding a remarkable connection between predicted secondary structure and aspects of function. Secondary structure predictions are increasingly becoming the work horse for numerous methods aimed at predicting protein structure and function. Is the recent increase in accuracy significant enough to make predictions even more useful? Because the recent improvement yields a better prediction of segments, and in particular of $\beta$ strands, I believe the answer is affirmative. What is the limit of prediction accuracy? We shall see. {\textcopyright} 2001 Academic Press.},
	author = {Rost, B.},
	doi = {10.1006/jsbi.2001.4336},
	journal = {J Struct Biol 134(2-3):204-18},
	keywords = {Secondary structure},
	mendeley-groups = {AML},
	mendeley-tags = {Secondary structure},
	pmid = {11551180},
	title = {Review: Protein secondary structure prediction continues to rise},
	year = {J Struct Biol 2001 May - 134(2-3): 204-18}
}

@article{Pauling1951,
	abstract = {During the past fifteen years we have been attacking the problem of the structure of proteins in several ways. One of these ways is the complete and accurate determination of the crystal structure of amino acids, peptides, and other simple substances related to proteins, in order that information about interatomic distances, bond angles, and other configurational parameters might be obtained that would permit the reliable prediction of reasonable configurations for the polypeptide chain. We have now used this information to construct two reasonable hydrogen-bonded helical configurations for the polypeptide chain; we think that it is likely that these configurations constitute an important part of the structure of both fibrous and globular proteins, as well as of synthetic polypeptides. A letter announcing their discovery was published last year.1 The problem that we have set ourselves is that of finding all hydrogen-bonded structures for a single polypeptide chain, in which the residues are equivalent (except for the differences in the side chain R). An amino acid residue (other than glycine) has no symmetry elements. The general operation of conversion of one residue of a single chain into a second residue equivalent to the first is accordingly a rotation about an axis accompanied by translation along the axis. Hence the only configurations for a chain compatible with our postulate of equivalence of the residues are helical configurations. For rotational angle 180° the helical configurations may degenerate to a simple chain with all of the principal atoms, C, C′ (the carbonyl carbon), N, and O, in the same plane. We assume that, because of the resonance of the double bond between the carbon-oxygen and carbon-nitrogen positions, the configuration of each residue is planar. This structural feature has been verified for each of the amides that we have studied. Moreover, the resonance theory is now so well grounded and its experimental substantiation so extensive that there can be no doubt whatever about its application to the amide group. The observed C—N distance, 1.32 {\AA}, corresponds to nearly 50 per cent double-bond character, and we may conclude that rotation by as much as 10° from the planar configuration would result in instability by about 1 kcal. mole−1. The interatomic distances and bond angles within the residue are assumed to have the values shown in figure 1. These values have been formulated2 by consideration of the experimental values found in the crystal structure studies of DL-alanine,3 L-threonine,4 N-acetylglycine5, and $\beta$-glycylglycine6 that have been made in our Laboratories. It is further assumed that each nitrogen atom forms a hydrogen bond with an oxygen atom of another residue, with the nitrogen-oxygen distance equal to 2.72 {\AA}, and that the vector from the nitrogen atom to the hydrogen-bonded oxygen atom lies not more than 30° from the N—H direction. The energy of an N—H {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} hydrogen bond is of the order of 8 kcal. mole−1, and such great instability would result from the failure to form these bonds that we may be confident of their presence. The N—H {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} O distance cannot be expected to be exactly 2.72 {\AA}, but might deviate somewhat from this value. View larger version: In this page In a new window Download as PowerPoint Slide Figure 1. Dimensions of the polypeptide chain. Solution of this problem shows that there are five and only five configurations for the chain that satisfy the conditions other than that of direction of the hydrogen bond relative to the N—H direction. These correspond to the values 165°, 120°, 108°, 97.2° and 70.1° for the rotational angle. In the first, third, and fifth of these structures the group is negatively and the group positively directed along the helical axis, taken as the direction corresponding to the sequence—CHR—CO—NH—CHR—of atoms in the peptide chain, and in the other two their directions are reversed. The first three of the structures are unsatisfactory, in that the N—H group does not extend in the direction of the oxygen atom at 2.72 {\AA}; the fourth and fifth are satisfactory, the angle between the N—H vector and N—O vector being about 10° and 25° for these two structures respectively. The fourth structure has 3.69 amino acid residues per turn in the helix, and the fifth structure has 5.13 residues per turn. In the fourth structure each amide group is hydrogen-bonded to the third amide group beyond it along the helix, and in the fifth structure each is bonded to the fifth amide group beyond it; we shall call these structures either the 3.7-residue structure and the 5.1-residue structure, respectively, or the third-amide hydrogen-bonded structure and the fifth-amide hydrogen-bonded structure. Drawings of the two structures are shown in figures 2, 3, 4, and 5. View larger version: In this page In a new window Download as PowerPoint Slide Figure 2. The helix with 3.7 residues per turn. View larger version: In this page In a new window Download as PowerPoint Slide Figure 3. The helix with 5.1 residues per turn. View larger version: In this page In a new window Download as PowerPoint Slide Figure 4. Plan of the 3.7-residue helix. View larger version: In this page In a new window Download as PowerPoint Slide Figure 5. Plan of the 5.1-residue helix. For glycine both the 3.7-residue helix and the 5.1-residue helix could occur with either a positive or a negative rotational translation; that is, as either a positive or a negative helix, relative to the positive direction of the helical axis given by the sequence of atoms in the peptide chain. For other amino acids with the L configuration, however, the positive helix and the negative helix would differ in the position of the side chains, and it might well be expected that in each case one sense of the helix would be more stable than the other. An arbitrary assignment of the R groups has been made in the figures. The translation along the helical axis in the 3.7-residue helix is 1.47 {\AA}, and that in the 5.1-residue helix is 0.99 {\AA}. The values for one complete turn are 5.44 {\AA} and 5.03 {\AA}, respectively. These values are calculated for the hydrogen-bond distance 2.72 {\AA}; they would have to be increased by a few per cent, in case that a larger hydrogen-bond distance (2.80 {\AA}, say) were present. The stability of our helical structures in a non-crystalline phase depends solely on interactions between adjacent residues, and does not require that the number of residues per turn be a ratio of small integers. The value 3.69 residues per turn, for the third-amide hydrogen-bonded helix, is most closely approximated by 48 residues in thirteen turns (3.693 residues per turn), and the value 5.13 for the other helix is most closely approximated by 41 residues in eight turns. It is to be expected that the number of residues per turn would be affected somewhat by change in the hydrogen-bond distance, and also that the interaction of helical molecules with neighboring similar molecules in a crystal would cause small torques in the helixes, deforming them slightly into configurations with a rational number of residues per turn. For the third-amide hydrogen-bonded helix the simplest structures of this sort that we would predict are the 11-residue, 3-turn helix (3.67 residues per turn), the 15-residue, 4-turn helix (3.75), and the 18-residue, 5-turn helix (3.60). We have found some evidence indicating that the first and third of these slight variants of this helix exist in crystalline polypeptides. These helical structures have not previously been described. In addition to the extended polypeptide chain configuration, which for nearly thirty years has been assumed to be present in stretched hair and other proteins with the $\beta$-keratin structure, configurations for the polypeptide chain have been proposed by Astbury and Bell,7 and especially by Huggins8 and by Bragg, Kendrew, and Perutz.9 Huggins discussed a number of structures involving intramolecular hydrogen bonds, and Bragg, Kendrew, and Perutz extended the discussion to include additional structures, and investigated the compatibility of the structures with x-ray diffraction data for hemoglobin and myoglobin. None of these authors proposed either our 3.7-residue helix or our 5.1-residue helix. On the other hand, we would eliminate, by our basic postulates, all of the structures proposed by them. The reason for the difference in results obtained by other investigators and by us through essentially similar arguments is that both Bragg and his collaborators and Huggins discussed in detail only helical structures with an integral number of residues per turn, and moreover assumed only a rough approximation to the requirements about interatomic distances, bond angles, and planarity of the conjugated amide group, as given by our investigations of simpler substances. We contend that these stereochemical features must be very closely retained in stable configurations of polypeptide chains in proteins, and that there is no special stability associated with an integral number of residues per turn in the helical molecule. Bragg, Kendrew, and Perutz have described a structure topologically similar to our 3.7-residue helix as a hydrogen-bonded helix with 4 residues per turn. In their thorough comparison of their models with Patterson projections for hemoglobin and myoglobin they eliminated this structure, and drew the cautious conclusion that the evidence favors the non-helical 3-residue folded $\alpha$-keratin configuration of Astbury and Bell, in which only one-third of the carbonyl and amino groups are involved in intramolecular hydrogen-bond formation. It is our opinion that the structure of $\alpha$-keratin, $\alpha$-myosin, and similar fibrous proteins is closely represented by our 3.7-residue helix, and that this helix also constitutes an important structural feature in hemoglobin, myoglobin, and other globular proteins, as well as of synthetic polypeptides. We think that the 5.1-residue helix may be represente},
	author = {Pauling, L. and Corey, R. B. and Branson, H. R.},
	doi = {10.1073/pnas.37.4.205},
	isbn = {0027-8424$\backslash$r1091-6490},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {14816373},
	title = {{The structure of proteins: Two hydrogen-bonded helical configurations of the polypeptide chain}},
	year = {1951}
}

@misc{Dill2012,
	abstract = {The protein-folding problem was first posed about one half-century ago. The term refers to three broad questions: (i) What is the physical code by which an amino acid sequence dictates a protein's native structure? (ii) How can proteins fold so fast? (iii) Can we devise a computer algorithm to predict protein structures from their sequences? We review progress on these problems. In a few cases, computer simulations of the physical forces in chemically detailed models have now achieved the accurate folding of small proteins. We have learned that proteins fold rapidly because random thermal motions cause conformational changes leading energetically downhill toward the native structure, a principle that is captured in funnel-shaped energy landscapes. And thanks in part to the large Protein Data Bank of known structures, predicting protein structures is now far more successful than was thought possible in the early days. What began as three questions of basic science one half-century ago has now grown into the full-fledged research field of protein physical science.},
	author = {Dill, K. A. and MacCallum, J. L.},
	booktitle = {Science},
	doi = {10.1126/science.1219021},
	isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
	issn = {10959203},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {23180855},
	title = {{The protein-folding problem, 50 years on}},
	year = {2012}
}

@article{Fauchere1988,
	abstract = {Fifteen physicochemical descriptors of side chains of the 20 natural and of 26 non-coded amino acids are compiled and simple methods for their evaluation described. The relevance of these parameters to account for hydrophobic, steric, and electric properties of the side chains is assessed and their intercorrelation analyzed. It is shown that three principal components, one steric, one bulk, and one electric (electronic), account for 66{\%} of the total variance in the available set. These parameters may prove to be useful for correlation studies in series of bioactive peptide analogues.},
	author = {Fauch{\`{e}}re, Jean-Luc and Charton, Marvin and Kier, Lemont B and Verloop, Arie and Pliska, Vladimir},
	doi = {10.1111/j.1399-3011.1988.tb01261.x},
	isbn = {1399-3011},
	issn = {0367-8377},
	journal = {International Journal of Peptide and Protein Research},
	keywords = {LFER parameters,QSAR in peptides,QSAR parameters,amino acid side chain parameters},
	pmid = {3209351},
	title = {{Amino acid side chain parameters for correlation studies in biology and pharmacology}},
	year = {1988}
}

@misc{Altschul1997,
	abstract = {The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.},
	archivePrefix = {arXiv},
	arxivId = {q-bio/0605018},
	author = {Altschul, Stephen F. and Madden, Thomas L. and Sch{\"{a}}ffer, Alejandro A. and Zhang, Jinghui and Zhang, Zheng and Miller, Webb and Lipman, David J.},
	booktitle = {Nucleic Acids Research},
	doi = {10.1093/nar/25.17.3389},
	eprint = {0605018},
	isbn = {10.1093/nar/25.17.3389},
	issn = {03051048},
	mendeley-groups = {Deep Biology},
	pmid = {9254694},
	primaryClass = {q-bio},
	title = {{Gapped BLAST and PSI-BLAST: A new generation of protein database search programs}},
	year = {1997}
}

@article{Jones1999,
	abstract = {A two-stage neural network has been used to predict protein secondary structure based on the position specific scoring matrices generated by PSI-BLAST. Despite the simplicity and convenience of the approach used, the results are found to be superior to those produced by other methods, including the popular PHD method according to our own benchmarking results and the results from the recent Critical Assessment of Techniques for Protein Structure Prediction experiment (CASP3), where the method was evaluated by stringent blind testing. Using a new testing set based on a set of 187 unique folds, and three-way cross-validation based on structural similarity criteria rather than sequence similarity criteria used previously (no similar folds were present in both the testing and training sets) the method presented here (PSIPRED) achieved an average Q(3) score of between 76.5{\%} to 78.3{\%} depending on the precise definition of observed secondary structure used, which is the highest published score for any method to date. Given the success of the method in CASP3, it is reasonable to be confident that the evaluation presented here gives a fair indication of the performance of the method in general. (C) 1999 Academic Press.},
	archivePrefix = {arXiv},
	arxivId = {0-387-31073-8},
	author = {Jones, D. T.},
	doi = {10.1006/jmbi.1999.3091},
	eprint = {0-387-31073-8},
	isbn = {0022-2836 (Print) 0022-2836 (Linking)},
	issn = {0022-2836},
	journal = {J. Mol. Biol.},
	keywords = {QD Chemistry,Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {10493868},
	title = {{Protein secondary structure prediction based on position-specific scoring matrices}},
	year = {1999}
}

@article{Mesnil2015,
	abstract = {Semantic slot filling is one of the most challenging problems in spoken language understanding (SLU). In this paper, we propose to use recurrent neural networks (RNNs) for this task, and present several novel architectures designed to efficiently model past and future temporal dependencies. Specifically, we implemented and compared several important RNN architectures, including Elman, Jordan, and hybrid variants. To facilitate reproducibility, we implemented these networks with the publicly available Theano neural network toolkit and completed experiments on the well-known airline travel information system (ATIS) benchmark. In addition, we compared the approaches on two custom SLU data sets from the entertainment and movies domains. Our results show that the RNN-based models outperform the conditional random field (CRF) baseline by 2{\%} in absolute error reduction on the ATIS benchmark. We improve the state-of-the-art by 0.5{\%} in the Entertainment domain, and 6.7{\%} for the movies domain.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1406.2661v1},
	author = {Mesnil, Gregoire and Dauphin, Yann and Yao, Kaisheng and Bengio, Yoshua and Deng, Li and Hakkani-Tur, Dilek and He, Xiaodong and Heck, Larry and Tur, Gokhan and Yu, Dong and Zweig, Geoffrey},
	doi = {10.1109/TASLP.2014.2383614},
	eprint = {arXiv:1406.2661v1},
	isbn = {9781139058452},
	issn = {2329-9290},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	keywords = {Context,Elman architecture,Filling,Hidden Markov models,Jordan architecture,Recurrent neural network (RNN),Recurrent neural networks,Semantics,Theano neural network toolkit,Vectors,airline travel information system,recurrent neural nets,recurrent neural network,semantic slot filling,slot filling,speech recognition,spoken language understanding,spoken language understanding (SLU),word embedding},
	pmid = {1000183096},
	title = {{Using Recurrent Neural Networks for Slot Filling in Spoken Language Understanding}},
	year = {2015}
}

@article{Qian1988,
	abstract = {We present a new method for predicting the secondary structure of globular proteins based on non-linear neural network models. Network models learn from existing protein structures how to predict the secondary structure of local sequences of amino acids. The average success rate of our method on a testing set of proteins non-homologous with the corresponding training set was 64.3{\%} on three types of secondary structure ($\alpha$-helix, $\beta$-sheet, and coil), with correlation coefficients of C$\alpha$= 0.41, C$\beta$= 0.31 and Ccoll= 0.41. These quality indices are all higher than those of previous methods. The prediction accuracy for the first 25 residues of the N-terminal sequence was significantly better. We conclude from computational experiments on real and artificial structures that no method based solely on local information in the protein sequence is likely to produce significantly better results for non-homologous proteins. The performance of our method of homologous proteins is much better than for non-homologous proteins, but is not as good as simply assuming that homologous sequences have identical structures. {\textcopyright} 1988.},
	author = {Qian, Ning and Sejnowski, Terrence J.},
	doi = {10.1016/0022-2836(88)90564-5},
	isbn = {0022-2836},
	issn = {00222836},
	journal = {Journal of Molecular Biology},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {3172241},
	title = {{Predicting the secondary structure of globular proteins using neural network models}},
	year = {1988}
}

@article{Wang2011,
	abstract = {Compared with the protein 3-class secondary structure (SS) prediction, the 8-class prediction gains less attention and is also much more challenging, especially for proteins with few sequence homologs. This paper presents a new probabilistic method for 8-class SS prediction using conditional neural fields (CNFs), a recently invented probabilistic graphical model. This CNF method not only models the complex relationship between sequence features and SS, but also exploits the interdependency among SS types of adjacent residues. In addition to sequence profiles, our method also makes use of non-evolutionary information for SS prediction. Tested on the CB513 and RS126 data sets, our method achieves Q8 accuracy of 64.9 and 64.7{\%}, respectively, which are much better than the SSpro8 web server (51.0 and 48.0{\%}, respectively). Our method can also be used to predict other structure properties (e.g. solvent accessibility) of a protein or the SS of RNA.},
	archivePrefix = {arXiv},
	arxivId = {NIHMS150003},
	author = {Wang, Z. and Zhao, F. and Peng, J. and Xu, J.},
	doi = {10.1002/pmic.201100196},
	eprint = {NIHMS150003},
	isbn = {9781424483075},
	issn = {16159853},
	journal = {Proteomics},
	keywords = {Bioinformatics,Conditional neural fields,Eight class,Protein,Secondary structure,Secondary structure prediction},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {21805636},
	title = {{Protein 8-class secondary structure prediction using conditional neural fields}},
	year = {2011}
}

@article{Spencer2015,
	abstract = {Ab initio protein secondary structure (SS) predictions are utilized to generate tertiary structure predictions, which are increasingly demanded due to the rapid discovery of proteins. Although recent developments have slightly exceeded previous methods of SS prediction, accuracy has stagnated around 80 percent and many wonder if prediction cannot be advanced beyond this ceiling. Disciplines that have traditionally employed neural networks are experimenting with novel deep learning techniques in attempts to stimulate progress. Since neural networks have historically played an important role in SS prediction, we wanted to determine whether deep learning could contribute to the advancement of this field as well. We developed an SS predictor that makes use of the position-specific scoring matrix generated by PSI-BLAST and deep learning network architectures, which we call DNSS. Graphical processing units and CUDA software optimize the deep network architecture and efficiently train the deep networks. Optimal parameters for the training process were determined, and a workflow comprising three separately trained deep networks was constructed in order to make refined predictions. This deep learning network approach was used to predict SS for a fully independent test dataset of 198 proteins, achieving a Q3 accuracy of 80.7 percent and a Sov accuracy of 74.2 percent.},
	annote = {Deep MLP pre-trained with RBM},
	author = {Spencer, M. and Eickholt, J. and Cheng, J.},
	doi = {10.1109/TCBB.2014.2343960},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spencer, Eickholt, Cheng - 2015 - A Deep Learning Network Approach to ab initio Protein Secondary Structure Prediction.pdf:pdf},
	isbn = {1545-5963 VO - 12},
	issn = {1545-5963},
	journal = {Ieee/Acm Transactions on Computational Biology and Bioinformatics},
	volume = {12},
	number = {1},
	pages = {103-112},
	keywords = {Deep learning,Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Deep learning,Secondary structure},
	pmid = {25750595},
	title = {{A Deep Learning Network Approach to ab initio Protein Secondary Structure Prediction}},
	year = {2015}
}

@article{Magnan2014,
	abstract = {MOTIVATION Accurately predicting protein secondary structure and relative solvent accessibility is important for the study of protein evolution, structure and function and as a component of protein 3D structure prediction pipelines. Most predictors use a combination of machine learning and profiles, and thus must be retrained and assessed periodically as the number of available protein sequences and structures continues to grow. RESULTS We present newly trained modular versions of the SSpro and ACCpro predictors of secondary structure and relative solvent accessibility together with their multi-class variants SSpro8 and ACCpro20. We introduce a sharp distinction between the use of sequence similarity alone, typically in the form of sequence profiles at the input level, and the additional use of sequence-based structural similarity, which uses similarity to sequences in the Protein Data Bank to infer annotations at the output level, and study their relative contributions to modern predictors. Using sequence similarity alone, SSpro's accuracy is between 79 and 80{\%} (79{\%} for ACCpro) and no other predictor seems to exceed 82{\%}. However, when sequence-based structural similarity is added, the accuracy of SSpro rises to 92.9{\%} (90{\%} for ACCpro). Thus, by combining both approaches, these problems appear now to be essentially solved, as an accuracy of 100{\%} cannot be expected for several well-known reasons. These results point also to several open technical challenges, including (i) achieving on the order of ≥ 80{\%} accuracy, without using any similarity with known proteins and (ii) achieving on the order of ≥ 85{\%} accuracy, using sequence similarity alone. AVAILABILITY AND IMPLEMENTATION SSpro, SSpro8, ACCpro and ACCpro20 programs, data and web servers are available through the SCRATCH suite of protein structure predictors at http://scratch.proteomics.ics.uci.edu.},
	author = {Magnan, C. N. and Baldi, P.},
	doi = {10.1093/bioinformatics/btu352},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Magnan, Baldi - 2014 - SSproACCpro 5 Almost perfect prediction of protein secondary structure and relative solvent accessibility using p.pdf:pdf},
	isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
	issn = {14602059},
	journal = {Bioinformatics},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {24860169},
	title = {{SSpro/ACCpro 5: Almost perfect prediction of protein secondary structure and relative solvent accessibility using profiles, machine learning and structural similarity}},
	year = {2014}
}

@article{Yang2018,
	abstract = {Protein secondary structure prediction began in 1951 when Pauling and Corey predicted helical and sheet conformations for protein polypeptide backbone even before the first protein structure was determined. Sixty-five years later, powerful new methods breathe new life into this field. The highest three-state accuracy without relying on structure templates is now at 82–84{\%}, a number unthinkable just a few years ago. These improvements came from increasingly larger databases of pro- tein sequences and structures for training, the use of template secondary structure information and more powerful deep learning techniques. As we are approaching to the theoretical limit of three-state prediction (88–90{\%}), alternative to second- ary structure prediction (prediction of backbone torsion angles and Ca-atom-based angles and torsion angles) not only has more room for further improvement but also allows direct prediction of three-dimensional fragment structures with con- stantly improved accuracy. About 20{\%} of all 40-residue fragments in a database of 1199 non-redundant proteins have {\textless}6A ˚ root-mean-squared distance from the native conformations by SPIDER2. More powerful deep learning methods with im- proved capability of capturing long-range interactions begin to emerge as the next generation of techniques for secondary structure prediction. The time has come to finish off the final stretch of the long march towards protein secondary structure prediction.},
	author = {Yang, Y. and Gao, J. and Wang, J. and Heffernan, R. and Hanson, J. and Paliwal, K. and Zhou, Y.},
	doi = {10.1093/bib/bbw129},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2018 - Sixty-five years of the long march in protein secondary structure prediction The final stretch.pdf:pdf},
	issn = {14774054},
	journal = {Briefings in Bioinformatics},
	volume = {19(3)},
	pages = {482-494},
	keywords = {Backbone structure prediction,Broad,Deep neural networks,Machine learning,Secondary structure,Secondary structure prediction,Torsion angle prediction},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Broad,Secondary structure},
	pmid = {28040746},
	title = {{Sixty-five years of the long march in protein secondary structure prediction: The final stretch?}},
	year = {2018}
}

@article{Cybenko1989,
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	author = {Cybenko, George},
	doi = {10.1007/BF02836480},
	isbn = {0780300564},
	issn = {10009221},
	journal = {Approximation Theory and its Applications},
	mendeley-groups = {AML},
	title = {{Approximations by superpositions of sigmoidal functions}},
	year = {1989}
}

@article{Barron1993,
	abstract = {Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The function approximated is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n2/d uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined here, the approximation rate and the parsimony of the parameterization of the networks are surprisingly advantageous in high-dimensional settings.},
	author = {Barron, Andrew R.},
	doi = {10.1109/18.256500},
	isbn = {0-7803-0056-4},
	issn = {00189448},
	journal = {IEEE Transactions on Information Theory},
	mendeley-groups = {AML},
	pmid = {18255629},
	title = {{Universal approximation bounds for superpositions of a sigmoidal function}},
	year = {1993}
}


@article{doi:10.1016/0014-5793(88)81066-4,
	author = {Bohr, H. and Bohr, J. and Brunak, S. and Cotterill, R. M.J. and Lautrup, B. and Nørskov, L. and Olsen, O. H. and Petersen, S. B.},
	title = {Protein secondary structure and homology by neural networks The α-helices in rhodopsin},
	journal = {FEBS Letters},
	volume = {241},
	number = {1-2},
	pages = {223-228},
	keywords = {Secondary structure, Neural network, Protein folding, Rhodopsin α-helix, Protein homology, Perceptron},
	doi = {10.1016/0014-5793(88)81066-4},
	url = {https://febs.onlinelibrary.wiley.com/doi/abs/10.1016/0014-5793%2888%2981066-4},
	eprint = {https://febs.onlinelibrary.wiley.com/doi/pdf/10.1016/0014-5793%2888%2981066-4},
	abstract = {Neural networks provide a basis for semiempirical studies of pattern matching between the primary and secondary structures of proteins. Networks of the perceptron class have been trained to classify the amino-acid residues into two categories for each of three types of secondary feature: α-helix or not, β-sheet or not, and random coil or not. The explicit prediction for the helices in rhodopsin is compared with both electron microscopy results and those of the Chou-Fasman method. A new measure of homology between proteins is provided by the network approach, which thereby leads to quantification of the differences between the primary structures of proteins.},
	year={1988},
}

@article{Rost1993,
	abstract = {The explosive accumulation of protein sequences in the wake of large-scale sequencing projects is in stark contrast to the much slower experimental determination of protein structures. Improved methods of structure prediction from the gene sequence alone are therefore needed. Here, we report a substantial increase in both the accuracy and quality of secondary-structure predictions, using a neural-network algorithm. The main improvements come from the use of multiple sequence alignments (better overall accuracy), from "balanced training" (better prediction of beta-strands), and from "structure context training" (better prediction of helix and strand lengths). This method, cross-validated on seven different test sets purged of sequence similarity to learning sets, achieves a three-state prediction accuracy of 69.7{\%}, significantly better than previous methods. In addition, the predicted structures have a more realistic distribution of helix and strand segments. The predictions may be suitable for use in practice as a first estimate of the structural type of newly sequenced proteins.},
	author = {Rost, B. and Sander, C.},
	doi = {10.1073/pnas.90.16.7558},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {8356056},
	title = {{Improved prediction of protein secondary structure by use of sequence profiles and neural networks.}},
	year = {1993}
}

@article{Dor2007,
	abstract = {The crystal structure of the Midwest Center for Structural Genomics target APC35832, a 14.7-kDa cytosolic protein from Bacillus stearothermophilus, has been determined at 1.3 A resolution by the single anomalous diffraction method from a mercury soaked crystal. The APC35832 protein is a representative of large group of bacterial and archeal proteins entirely consisting of the Toprim (topoisomerase-primase) domain. This domain is found in the catalytic centers of many enzymes catalyzing phosphodiester bond formation or cleavage, but the function of small Toprim domain proteins remains unknown. Consistent with the sequence analysis, the APC35832 structure shows a conserved Toprim fold, with a central 4-stranded parallel beta-sheet surrounded by four alpha-helixes. Comparison of the APC35832 structure with its closest structural homolog, the catalytic core of bacteriophage T7 primase, revealed structural conservation of a metal binding site and isothermal titration calorimetry indicates that APC35832 binds Mg2+ with a sub-millimolar dissociation constant (K(d)). The APC35832-Mg2+ complex structure was determined at 1.65 A and reveals the role of conserved acidic residues in Mg2+ ion coordination. The structural similarities to other Toprim domain containing proteins and potential function and substrates of APC35832 are discussed in this article.},
	archivePrefix = {arXiv},
	arxivId = {q-bio/0605018},
	author = {Dor, Ofer and Zhou, Yaoqi},
	doi = {10.1002/prot.21298},
	eprint = {0605018},
	isbn = {0887-3585},
	issn = {08873585},
	journal = {Proteins: Structure, Function and Genetics},
	keywords = {Neural network,Secondary structure,Solvent accessibility,Solvent accessible surface area},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {17705269},
	primaryClass = {q-bio},
	title = {{Achieving 80{\%} ten-fold cross-validated accuracy for secondary structure prediction by large-scale training}},
	year = {2007}
}

@incollection{NIPS2012_4824,
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems 25},
	editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
	pages = {1097--1105},
	year = {2012},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{Cho2014,
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	archivePrefix = {arXiv},
	arxivId = {1406.1078},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	eprint = {1406.1078},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
	mendeley-groups = {AML},
	month = {jun},
	title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	year = {2014}
}

@article{Zhang2015,
	abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
	archivePrefix = {arXiv},
	arxivId = {1509.01626},
	author = {Zhang, X. and Zhao, J. and LeCun, Y.},
	eprint = {1509.01626},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhao, LeCun - 2015 - Character-level Convolutional Networks for Text Classification.pdf:pdf},
	keywords = {CNN},
	mendeley-groups = {AML},
	mendeley-tags = {CNN},
	month = {sep},
	title = {{Character-level Convolutional Networks for Text Classification}},
	url = {http://arxiv.org/abs/1509.01626},
	year = {2015}
}

@inproceedings{Vesely2013,
	abstract = {Sequence-discriminative training of deep neural networks (DNNs) is investigated on a 300 hour American English conversational telephone speech task. Different sequence-discriminative criteria - maximum mutual information (MMI), minimum phone error (MPE), state-level minimum Bayes risk (sMBR), and boosted MMI - are compared. Two different heuristics are investigated to improve the performance of the DNNs trained using sequence-based criteria - lattices are re-generated after the first iteration of training; and, for MMI and BMMI, the frames where the numerator and denominator hypotheses are disjoint are removed from the gradient computation. Starting from a competitive DNN baseline trained using cross-entropy, different sequence-discriminative criteria are shown to lower word error rates by 8-9{\%} relative, on average. Little difference is noticed between the different sequence-based criteria that are investigated. The experiments are done using the open-source Kaldi toolkit, which makes it possible for the wider community to reproduce these results.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Vesel{\'{y}}, Karel and Ghoshal, Arnab and Burget, Luk{\'{a}}{\v{s}} and Povey, Daniel},
	booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	doi = {10.1109/ICASSP.2013.6639310},
	eprint = {arXiv:1011.1669v3},
	isbn = {9781479927562},
	issn = {19909772},
	keywords = {Deep learning,Neural networks,RNN,Reproducible research,Sequence-criterion training,Speech recognition,speech recognition},
	mendeley-groups = {AML},
	mendeley-tags = {RNN,speech recognition},
	pmid = {25246403},
	title = {{Sequence-discriminative training of deep neural networks}},
	year = {2013}
}

@article{Szegedy2016,
	abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
	archivePrefix = {arXiv},
	arxivId = {1602.07261},
	author = {Szegedy, C. and Ioffe, S. and Vanhoucke, Vincent and Alemi, Alex},
	doi = {10.1016/j.patrec.2014.01.008},
	eprint = {1602.07261},
	isbn = {0167-8655},
	issn = {01678655},
	keywords = {CNN},
	mendeley-groups = {AML},
	mendeley-tags = {CNN},
	pmid = {23064159},
	title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
	year = {2016}
}

@inproceedings{Huang2017,
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	archivePrefix = {arXiv},
	arxivId = {1608.06993},
	author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
	booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	doi = {10.1109/CVPR.2017.243},
	eprint = {1608.06993},
	isbn = {9781538604571},
	issn = {0002-9645},
	keywords = {CNN},
	mendeley-groups = {AML},
	mendeley-tags = {CNN},
	pmid = {211888},
	title = {{Densely connected convolutional networks}},
	year = {2017}
}

@article{Cuff1999,
	abstract = {A new dataset of 396 protein domains is developed and used to evaluate the performance of the protein secondary structure prediction algorithms DSC, PHD, NNSSP, and PREDATOR. The maximum theoretical Q3 accuracy for combination of these methods is shown to be 78{\%}. A simple consensus prediction on the 396 domains, with automatically generated multiple sequence alignments gives an average Q3 prediction accuracy of 72.9{\%}. This is a 1{\%} improvement over PHD, which was the best single method evaluated. Segment Overlap Accuracy (SOV) is 75.4{\%} for the consensus method on the 396-protein set. The secondary structure definition method DSSP defines 8 states, but these are reduced by most authors to 3 for prediction. Application of the different published 8- to 3-state reduction methods shows variation of over 3{\%} on apparent prediction accuracy. This suggests that care should be taken to compare methods by the same reduction method. Two new sequence datasets (CB513 and CB251) are derived which are suitable for cross-validation of secondary structure prediction methods without artifacts due to internal homology. A fully automatic World Wide Web service that predicts protein secondary structure by a combination of methods is available via http://barton.ebi.ac.uk/.},
	author = {Cuff, J a and Barton, G J},
	doi = {10.1002/(SICI)1097-0134(19990301)34:4<508::AID-PROT10>3.0.CO;2-4 [pii]},
	isbn = {0887-3585 (Print)$\backslash$r0887-3585 (Linking)},
	issn = {0887-3585},
	journal = {Proteins},
	keywords = {Secondary structure,bench-,combination of methods,protein,secondary structure predic-,tion},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {10081963},
	title = {{Evaluation and improvement of multiple sequence methods for protein secondary structure prediction.}},
	year = {1999}
}

@article{Sønderby2014,
	abstract = {Prediction of protein secondary structure from the amino acid sequence is a classical bioinformatics problem. Common methods use feed forward neural networks or SVMs combined with a sliding window, as these models does not naturally handle sequential data. Recurrent neural networks are an generalization of the feed forward neural network that naturally handle sequential data. We use a bidirectional recurrent neural network with long short term memory cells for prediction of secondary structure and evaluate using the CB513 dataset. On the secondary structure 8-class problem we report better performance (0.674) than state of the art (0.664). Our model includes feed forward networks between the long short term memory cells, a path that can be further explored.},
	archivePrefix = {arXiv},
	arxivId = {1412.7828},
	author = {S{\o}nderby, S. K. and Winther, O.},
	eprint = {1412.7828},
	journal = {arXiv:1412.7828},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	title = {{Protein Secondary Structure Prediction with Long Short Term Memory Networks}},
	year = {2014}
}

@misc{Berman2003,
	abstract = {In recognition of the growing international and interdisciplinary nature of structural biology, three organizations have formed a collaboration to oversee the newly formed worldwide Protein Data Bank (wwPDB; http://www.wwpdb.org/},
	author = {Berman, Helen and Henrick, Kim and Nakamura, Haruki},
	booktitle = {Nature Structural Biology},
	doi = {10.1038/nsb1203-980},
	isbn = {1072-8368 (Print)$\backslash$n1072-8368 (Linking)},
	issn = {10728368},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {14634627},
	title = {{Announcing the worldwide Protein Data Bank}},
	year = {2003}
}

@article{Wang2003,
	abstract = {PISCES is a public server for culling sets of protein sequences from the Protein Data Bank (PDB) by sequence identity and structural quality criteria. PISCES can provide lists culled from the entire PDB or from lists of PDB entries or chains provided by the user. The sequence identities are obtained from PSI-BLAST alignments with position-specific substitution matrices derived from the non-redundant protein sequence database. PISCES therefore provides better lists than servers that use BLAST, which is unable to identify many relationships below 40{\%} sequence identity and often overestimates sequence identity by aligning only well-conserved fragments. PDB sequences are updated weekly. PISCES can also cull non-PDB sequences provided by the user as a list of GenBank identifiers, a FASTA format file, or BLAST/PSI-BLAST output.},
	author = {Wang, G. and Dunbrack, R. L.},
	doi = {10.1093/bioinformatics/btg224},
	isbn = {1367-4803 (Print)$\backslash$r1367-4803 (Linking)},
	issn = {13674803},
	journal = {Bioinformatics},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {12912846},
	title = {{PISCES: A protein sequence culling server}},
	year = {2003}
}

@article{Rost1993,
	abstract = {We have trained a two-layered feed-forward neural network on a non-redundant data base of 130 protein chains to predict the secondary structure of water-soluble proteins. A new key aspect is the use of evolutionary information in the form of multiple sequence alignments that are used as input in place of single sequences. The inclusion of protein family information in this form increases the prediction accuracy by six to eight percentage points. A combination of three levels of networks results in an overall three state accuracy of 70.8{\%} for globular proteins (sustained performance). If four membrane protein chains are included in the evaluation, the overall accuracy drops to 70.2{\%}. The prediction is well balanced between $\alpha$-helix, $\beta$-strand and loop: 65{\%} of the observed strand residues are predicted correctly. The accuray in predicting the content of three secondary structure types is comparable to that of circular dichroism spectroscopy. The performance accuracy is verified by a sevenfold cross-validation test, and an additional test on 26 recently solved proteins. Of particular practical importance is the definition of a position-specific reliability index. For half of the residues predicted with a high level of reliability the overall accuracy increases to better than 82{\%}. A further strength of the method is the more realistic prediction of segment length. The protein family prediction method is available for testing by academic researchers via an electronic mail server.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Rost, B. and Sander, C.},
	doi = {10.1006/jmbi.1993.1413},
	eprint = {arXiv:1011.1669v3},
	isbn = {0022-2836 (Print)},
	issn = {00222836},
	journal = {Journal of Molecular Biology},
	keywords = {Multiple sequence alignments,Neural network,Protein secondary structure prediction,Secondary structure,Secondary structure content},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pmid = {8345525},
	title = {{Prediction of protein secondary structure at better than 70{\%} accuracy}},
	year = {1993}
}

@article{Kabsch1983,
	abstract = {For a successful analysis of the relation between amino acid sequence and protein structure, an unambiguous and physically meaningful definition of secondary structure is essential. We have developed a set of simple and physically motivated criteria for secondary structure, programmed as a pattern-recognition process of hydrogen-bonded and geometrical features extracted from x-ray coordinates. Cooperative secondary structure is recognized as repeats of the elementary hydrogen-bonding patterns “turn” and “bridge.” Repeating turns are “helices,” repeating bridges are “ladders,” connected ladders are “sheets.” Geometric structure is defined in terms of the concepts torsion and curvature of differential geometry. Local chain “chirality” is the torsional handedness of four consecutive C$\alpha$ positions and is positive for right-handed helices and negative for ideal twisted $\beta$-sheets. Curved pieces are defined as “bends.” Solvent “exposure” is given as the number of water molecules in possible contact with a residue. The end result is a compilation of the primary structure, including SS bonds, secondary structure, and solvent exposure of 62 different globular proteins. The presentation is in linear form: strip graphs for an overall view and strip tables for the details of each of 10.925 residues. The dictionary is also available in computer-readable form for protein structure prediction work.},
	archivePrefix = {arXiv},
	arxivId = {0006-3525/83/122577-6},
	author = {Kabsch, W. and Sander, C.},
	doi = {10.1002/bip.360221211},
	eprint = {83/122577-6},
	file = {:home/juillermo/Desktop/a810f3f43cd1aaeea04782ae4234cb61a3f8.pdf:pdf},
	isbn = {0006-3525 (Print) 0006-3525 (Linking)},
	issn = {10970282},
	journal = {Biopolymers},
	keywords = {Computational Biology,Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Computational Biology,Secondary structure},
	number = {12},
	pages = {2577--2637},
	pmid = {6667333},
	primaryClass = {0006-3525},
	title = {{Dictionary of protein secondary structure: Pattern recognition of hydrogen‐bonded and geometrical features}},
	volume = {22},
	year = {1983}
}

@article{Sundararajan2017,
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	archivePrefix = {arXiv},
	arxivId = {1703.01365},
	author = {Sundararajan, M. and Taly, A. and Yan, Q.},
	eprint = {1703.01365},
	journal = {arXiv:1703.01365},
	file = {:home/juillermo/Desktop/1703.01365.pdf:pdf},
	isbn = {9781510855144},
	issn = {1938-7228},
	keywords = {Feature visualization,Saliency maps},
	mendeley-groups = {AML},
	mendeley-tags = {Saliency maps,Feature visualization},
	title = {{Axiomatic Attribution for Deep Networks}},
	year = {2017}
}


@article{Zhou2014,
	abstract = {Predicting protein secondary structure is a fundamental problem in protein structure prediction. Here we present a new supervised generative stochastic network (GSN) based method to predict local secondary structure with deep hierarchical representations. GSN is a recently proposed deep learning technique (Bengio {\&} Thibodeau-Laufer, 2013) to globally train deep generative model. We present the supervised extension of GSN, which learns a Markov chain to sample from a conditional distribution, and applied it to protein structure prediction. To scale the model to full-sized, high-dimensional data, like protein sequences with hundreds of amino acids, we introduce a convolutional architecture, which allows efficient learning across multiple layers of hierarchical representations. Our architecture uniquely focuses on predicting structured low-level labels informed with both low and high-level representations learned by the model. In our application this corresponds to labeling the secondary structure state of each amino-acid residue. We trained and tested the model on separate sets of non-homologous proteins sharing less than 30{\%} sequence identity. Our model achieves 66.4{\%} Q8 accuracy on the CB513 dataset, better than the previously reported best performance 64.9{\%} (Wang et al., 2011) for this challenging secondary structure prediction problem.},
	archivePrefix = {arXiv},
	arxivId = {1403.1347},
	author = {Zhou, J. and Troyanskaya, O. G.},
	eprint = {1403.1347},
	journal = {Proceedings of the $31^{st}$ International Conference on Machine Learning. JMLR: W\&CP},
	volume={32},
	isbn = {9781634393973},
	keywords = {CNN,Deep learning,Secondary structure},
	mendeley-tags = {CNN,Deep learning,Secondary structure},
	title = {{Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction}},
	year = {2014}
}

@article{Kabsch1983,
	abstract = {For a successful analysis of the relation between amino acid sequence and protein structure, an unambiguous and physically meaningful definition of secondary structure is essential. We have developed a set of simple and physically motivated criteria for secondary structure, programmed as a pattern-recognition process of hydrogen-bonded and geometrical features extracted from x-ray coordinates. Cooperative secondary structure is recognized as repeats of the elementary hydrogen-bonding patterns “turn” and “bridge.” Repeating turns are “helices,” repeating bridges are “ladders,” connected ladders are “sheets.” Geometric structure is defined in terms of the concepts torsion and curvature of differential geometry. Local chain “chirality” is the torsional handedness of four consecutive C$\alpha$ positions and is positive for right-handed helices and negative for ideal twisted $\beta$-sheets. Curved pieces are defined as “bends.” Solvent “exposure” is given as the number of water molecules in possible contact with a residue. The end result is a compilation of the primary structure, including SS bonds, secondary structure, and solvent exposure of 62 different globular proteins. The presentation is in linear form: strip graphs for an overall view and strip tables for the details of each of 10.925 residues. The dictionary is also available in computer-readable form for protein structure prediction work.},
	archivePrefix = {arXiv},
	arxivId = {0006-3525/83/122577-6},
	author = {Kabsch, W. and Sander, C.},
	doi = {10.1002/bip.360221211},
	eprint = {83/122577-6},
	isbn = {0006-3525 (Print) 0006-3525 (Linking)},
	issn = {10970282},
	journal = {Biopolymers},
	keywords = {Computational Biology,Secondary structure},
	mendeley-tags = {Secondary structure,Computational Biology},
	number = {12},
	pages = {2577--2637},
	pmid = {6667333},
	primaryClass = {0006-3525},
	title = {{Dictionary of protein secondary structure: Pattern recognition of hydrogen‐bonded and geometrical features}},
	volume = {22},
	year = {1983}
}

@article{Jurtz2017,
	abstract = {Motivation: Deep neural network architectures such as convolutional and long short-term memory networks have become increasingly popular as machine learning tools during the recent years. The availability of greater computational resources, more data, new algorithms for training deep models and easy to use libraries for implementation and training of neural networks are the drivers of this development. The use of deep learning has been especially successful in image recognition; and the development of tools, applications and code examples are in most cases centered within this field rather than within biology. Results: Here, we aim to further the development of deep learning methods within biology by providing application examples and ready to apply and adapt code templates. Given such examples, we illustrate how architectures consisting of convolutional and long short-term memory neural networks can relatively easily be designed and trained to state-of-the-art performance on three biological sequence problems: prediction of subcellular localization, protein secondary structure and the binding of peptides to MHC Class II molecules. Availability: All implementations and datasets are available online to the scientific community at https://github.com/vanessajurtz/lasagne4bio. Supplementary information:Supplementary data are available at Bioinformatics online.},
	annote = {The key paper.},
	archivePrefix = {arXiv},
	arxivId = {103549},
	author = {Jurtz, V. I. and Johansen, A. R. and Nielsen, M. and {Almagro Armenteros}, J. J. and Nielsen, H. and S{\o}nderby, C. K. and Winther, O. and S{\o}nderby, S. K.},
	doi = {10.1093/bioinformatics/btx531},
	eprint = {103549},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jurtz et al. - 2017 - An introduction to deep learning on biological sequence data Examples and solutions.pdf:pdf},
	isbn = {1367-4811 (Electronic)},
	issn = {14602059},
	journal = {Bioinformatics},
	keywords = {Broad,CNN,Computational Biology,Deep learning,LSTM,RNN,Secondary structure,Sequencing},
	mendeley-tags = {Broad,CNN,Computational Biology,Deep learning,LSTM,RNN,Secondary structure,Sequencing},
	number = {22},
	pages = {3685--3690},
	pmid = {28961695},
	title = {{An introduction to deep learning on biological sequence data: Examples and solutions}},
	volume = {33},
	year = {2017}
}

@article{Ioffe2015,
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
	archivePrefix = {arXiv},
	arxivId = {1502.03167},
	author = {Ioffe, Sergey and Szegedy, Christian},
	eprint = {1502.03167},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
	month = {feb},
	title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
	url = {http://arxiv.org/abs/1502.03167},
	year = {2015}
}

@article{Thomsen2012,
	abstract = {Seq2Logo is a web-based sequence logo generator. Sequence logos are a graphical representation of the information content stored in a multiple sequence alignment (MSA) and provide a compact and highly intuitive representation of the position-specific amino acid composition of binding motifs, active sites, etc. in biological sequences. Accurate generation of sequence logos is often compromised by sequence redundancy and low number of observations. Moreover, most methods available for sequence logo generation focus on displaying the position-specific enrichment of amino acids, discarding the equally valuable information related to amino acid depletion. Seq2logo aims at resolving these issues allowing the user to include sequence weighting to correct for data redundancy, pseudo counts to correct for low number of observations and different logotype representations each capturing different aspects related to amino acid enrichment and depletion. Besides allowing input in the format of peptides and MSA, Seq2Logo accepts input as Blast sequence profiles, providing easy access for non-expert end-users to characterize and identify functionally conserved/variable amino acids in any given protein of interest. The output from the server is a sequence logo and a PSSM. Seq2Logo is available at http://www.cbs.dtu.dk/biotools/Seq2Logo (14 May 2012, date last accessed).},
	author = {Thomsen, M. C. F. and Nielsen, M.},
	doi = {10.1093/nar/gks469},
	isbn = {1362-4962 (Electronic) 0305-1048 (Linking)},
	issn = {03051048},
	journal = {Nucleic Acids Research},
	number = {W1},
	pmid = {22638583},
	title = {{Seq2Logo: A method for construction and visualization of amino acid binding motifs and sequence profiles including sequence weighting, pseudo counts and two-sided representation of amino acid enrichment and depletion}},
	volume = {40},
	year = {2012}
}


@article{Glorot2010,
	abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Glorot, Xavier and Bengio, Yoshua},
	doi = {10.1.1.207.2059},
	eprint = {arXiv:1011.1669v3},
	isbn = {9781937284275},
	issn = {15324435},
	journal = {PMLR},
	pages = {249--256},
	pmid = {25246403},
	title = {{Understanding the difficulty of training deep feedforward neural networks}},
	url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
	volume = {9},
	year = {2010}
}


@article{Avdagic2009,
	abstract = {In this paper we describe CB513 a non-redundant dataset, suitable for development of algorithms for prediction of secondary protein structure. A program was made in Borland Delphi for transforming data from our dataset to make it suitable for learning of neural network for prediction of secondary protein structure implemented in MATLAB Neural-Network Toolbox. Learning (training and testing) of neural network is researched with different sizes of windows, different number of neurons in the hidden layer and different number of training epochs, while using dataset CB513.},
	author = {Avdagic, Zikrija and Purisevic, Elvir and Omanovic, Samir and Coralic, Zlatan},
	issn = {2153-6430},
	journal = {Summit on translational bioinformatics},
	keywords = {Computational Biology,Secondary structure},
	mendeley-tags = {Secondary structure,Computational Biology},
	pages = {1--5},
	pmid = {21347158},
	title = {{Artificial Intelligence in Prediction of Secondary Protein Structure Using CB513 Database.}},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3041573{\&}tool=pmcentrez{\&}rendertype=abstract},
	volume = {2009},
	year = {2009}
}


@article{Zhou2015,
	abstract = {Identifying functional effects of noncoding variants is a major challenge in human genetics. To predict the noncoding-variant effects de novo from sequence, we developed a deep learning-based algorithmic framework, DeepSEA (http://deepsea.princeton.edu/), that directly learns a regulatory sequence code from large-scale chromatin-profiling data, enabling prediction of chromatin effects of sequence alterations with single-nucleotide sensitivity. We further used this capability to improve prioritization of functional variants including expression quantitative trait loci (eQTLs) and disease-associated variants.},
	archivePrefix = {arXiv},
	arxivId = {15334406},
	author = {Zhou, J. and Troyanskaya, O. G.},
	doi = {10.1038/nmeth.3547},
	eprint = {15334406},
	isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
	issn = {15487105},
	journal = {Nature Methods},
	keywords = {Computational Biology},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Computational Biology},
	number = {10, 931--4},
	pmid = {26301843},
	title = {{Predicting effects of noncoding variants with deep learning-based sequence model}},
	volume = {12},
	year = {2015}
}


@article{Shrikumar2017,
	abstract = {The purported "black box"' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
	archivePrefix = {arXiv},
	arxivId = {1704.02685},
	author = {Shrikumar, A. and Greenside, P. and Kundaje, A.},
	eprint = {1704.02685},
	journal = {arXiv:1704.02685},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrikumar, Greenside, Kundaje - 2017 - Learning Important Features Through Propagating Activation Differences.pdf:pdf},
	keywords = {Feature visualization,Saliency maps},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Saliency maps,Feature visualization},
	month = {apr},
	title = {{Learning Important Features Through Propagating Activation Differences}},
	url = {http://arxiv.org/abs/1704.02685},
	year = {2017}
}


@book{Ching2017,
	abstract = {Deep learning, which describes a class of machine learning algorithms, has recently showed impressive results across a variety of domains. Biology and medicine are data rich, but the data are complex and often ill-understood. Problems of this nature may be particularly well-suited to deep learning techniques. We examine applications of deep learning to a variety of biomedical problems -- patient classification, fundamental biological processes, and treatment of patients -- to predict whether deep learning will transform these tasks or if the biomedical sphere poses unique challenges. We find that deep learning has yet to revolutionize or definitively resolve any of these problems, but promising advances have been made on the prior state of the art. Even when improvement over a previous baseline has been modest, we have seen signs that deep learning methods may speed or aid human investigation. More work is needed to address concerns related to interpretability and how to best model each problem. Furthermore, the limited amount of labeled data for training presents problems in some domains, as can legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning powering changes at the bench and bedside with the potential to transform several areas of biology and medicine.},
	archivePrefix = {arXiv},
	arxivId = {142760},
	author = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Harris, David J. and DeCaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H. S. and Gitter, Anthony and Greene, Casey S.},
	booktitle = {bioRxiv},
	doi = {10.1101/142760},
	eprint = {142760},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ching et al. - 2017 - Opportunities And Obstacles For Deep Learning In Biology And Medicine.pdf:pdf},
	isbn = {0000000305396},
	keywords = {Broad},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Broad},
	title = {{Opportunities And Obstacles For Deep Learning In Biology And Medicine}},
	year = {2017}
}


@article{Montavon2017,
	abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
	archivePrefix = {arXiv},
	arxivId = {1512.02479},
	author = {Montavon, G. and Lapuschkin, S. and Binder, A. and Samek, W. and M{\"{u}}ller, K. R.},
	doi = {10.1016/j.patcog.2016.11.008},
	eprint = {1512.02479},
	file = {:home/juillermo/Desktop/1512.02479v1.pdf:pdf},
	isbn = {0031-3203},
	issn = {00313203},
	journal = {arXiv:1512.02479},
	keywords = {Deep neural networks,Feature visualization,Heatmapping,Image recognition,Relevance propagation,Taylor decomposition},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	title = {{Explaining nonlinear classification decisions with deep Taylor decomposition}},
	year = {2015}
}


@misc{Montavon2018,
	abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.},
	annote = {Fucking awesome},
	archivePrefix = {arXiv},
	arxivId = {1706.07979},
	author = {Montavon, G. and Samek, W. and M{\"{u}}ller, K. R.},
	booktitle = {Digital Signal Processing: A Review Journal},
	doi = {10.1016/j.dsp.2017.10.011},
	eprint = {1706.07979},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Montavon, Samek, M{\"{u}}ller - 2018 - Methods for interpreting and understanding deep neural networks(2).pdf:pdf},
	issn = {10512004},
	keywords = {Activation maximization,Deep neural networks,Feature visualization,Layer-wise relevance propagation,Sensitivity analysis,Taylor decomposition},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	title = {{Methods for interpreting and understanding deep neural networks}},
	year = {2018}
}

@article{Lanchantin2016,
	abstract = {Deep neural network (DNN) models have recently obtained state-of-the-art prediction accuracy for the transcription factor binding (TFBS) site classification task. However, it remains unclear how these approaches identify meaningful DNA sequence signals and give insights as to why TFs bind to certain locations. In this paper, we propose a toolkit called the Deep Motif Dashboard (DeMo Dashboard) which provides a suite of visualization strategies to extract motifs, or sequence patterns from deep neural network models for TFBS classification. We demonstrate how to visualize and understand three important DNN models: convolutional, recurrent, and convolutional-recurrent networks. Our first visualization method is finding a test sequence's saliency map which uses first-order derivatives to describe the importance of each nucleotide in making the final prediction. Second, considering recurrent models make predictions in a temporal manner (from one end of a TFBS sequence to the other), we introduce temporal output scores, indicating the prediction score of a model over time for a sequential input. Lastly, a class-specific visualization strategy finds the optimal input sequence for a given TFBS positive class via stochastic gradient optimization. Our experimental results indicate that a convolutional-recurrent architecture performs the best among the three architectures. The visualization techniques indicate that CNN-RNN makes predictions by modeling both motifs as well as dependencies among them.},
	archivePrefix = {arXiv},
	arxivId = {1608.03644},
	author = {Lanchantin, J. and Singh, R. and Wang, B. and Qi, Y.},
	doi = {10.1101/085191},
	eprint = {1608.03644},
	journal = {arXiv:1608.03644},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lanchantin et al. - 2016 - Deep Motif Dashboard Visualizing and Understanding Genomic Sequences Using Deep Neural Networks.pdf:pdf;:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lanchantin et al. - 2016 - Deep Motif Dashboard Visualizing and Understanding Genomic Sequences Using Deep Neural Networks(2).pdf:pdf},
	issn = {23356936},
	keywords = {Deep learning,Feature visualization},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Deep learning,Feature visualization},
	title = {{Deep Motif Dashboard: Visualizing and Understanding Genomic Sequences Using Deep Neural Networks}},
	year = {2016}
}


@misc{chollet2015keras,
	title={Keras},
	author={Chollet, Fran\c{c}ois and others},
	year={2015},
	howpublished={\url{https://keras.io}},
}

@phdthesis{Fontal2017,
	author = {Fontal, Alejandro},
	file = {:home/juillermo/Desktop/edepotair{\_}t5a2fb523{\_}001.pdf:pdf},
	keywords = {CNN,Computational Biology,Deep Learning,Feature visualization,LSTM,Machine Learning,RNN,Subcellular localization},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {CNN,Computational Biology,Deep Learning,LSTM,Machine Learning,RNN,Subcellular localization,Feature visualization},
	school = {Wageningen University {\&} Research},
	title = {{Neural Networks for Subcellular Localization Prediction}},
	url = {http://edepot.wur.nl/429151},
	year = {2017}
}


@inproceedings{Zeiler2014,
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	archivePrefix = {arXiv},
	arxivId = {1311.2901},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	doi = {10.1007/978-3-319-10590-1_53},
	eprint = {1311.2901},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Fergus - 2014 - Visualizing and understanding convolutional networks.pdf:pdf},
	isbn = {9783319105895},
	issn = {16113349},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	pmid = {26353135},
	title = {{Visualizing and understanding convolutional networks}},
	year = {2014}
}


@article{Szegedy2013,
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	archivePrefix = {arXiv},
	arxivId = {1312.6199},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	eprint = {1312.6199},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:pdf},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	month = {dec},
	title = {{Intriguing properties of neural networks}},
	url = {http://arxiv.org/abs/1312.6199},
	year = {2013}
}


@article{Gatys2016,
	abstract = {Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic in-formation and, thus, allow to separate image content from style. Here we use image representations derived from Con-volutional Neural Networks optimised for object recogni-tion, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can sep-arate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an ar-bitrary photograph with the appearance of numerous well-known artworks. Our results provide new insights into the deep image representations learned by Convolutional Neu-ral Networks and demonstrate their potential for high level image synthesis and manipulation.},
	archivePrefix = {arXiv},
	arxivId = {1505.07376},
	author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
	doi = {10.1109/CVPR.2016.265},
	eprint = {1505.07376},
	file = {:home/juillermo/Desktop/07780634.pdf:pdf},
	isbn = {9781467388511},
	issn = {10636919},
	journal = {The IEEE conference on computer vision and pattern recognition},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	pmid = {15430064963552939126},
	title = {{Image style transfer using convolutional neural networks}},
	year = {2016}
}


@inproceedings{Mahendran2015,
	abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
	archivePrefix = {arXiv},
	arxivId = {1412.0035},
	author = {Mahendran, Aravindh and Vedaldi, Andrea},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	doi = {10.1109/CVPR.2015.7299155},
	eprint = {1412.0035},
	file = {:home/juillermo/Desktop/1412.0035v1.pdf:pdf},
	isbn = {9781467369640},
	issn = {10636919},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	pmid = {903},
	title = {{Understanding deep image representations by inverting them}},
	year = {2015}
}

@article{Simonyan2014,
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	author = {Simonyan, K. and Vedaldi, A. and Zisserman, A.},
	file = {:home/juillermo/Desktop/1312.6034v2.pdf:pdf},
	journal = {arXiv:1312.6034},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
	year = {2014}
}

@misc{Mordvintsev2015,
	abstract = {Artificial Neural Networks have spurred remarkable recent progress in image classification and speech recognition. But even though these are very useful tools based on well-known mathematical methods, we actually understand surprisingly little of why certain models work and others don't. So let's take a look at some simple techniques for peeking inside these networks. We train an artificial neural network by showing it millions of training examples and gradually adjusting the network parameters until it gives the classifications we want. The network typically consists of 10-30 stacked layers of artificial neurons. Each image is fed into the input layer, which then talks to the next layer, until eventually the “output” layer is reached. The network's “answer” comes from this final output layer. One of the challenges of neural networks is understanding what exactly goes on at each layer. We know that after training, each layer progressively extracts higher and higher-level features of the image, until the final layer essentially makes a decision on what the image shows. For example, Artificial Neural Networks have spurred remarkable recent progress in image classification and speech recognition. But even though these are very useful tools based on well-known mathematical methods, we actually understand surprisingly little of why certain models work and others don't. So let's take a look at some simple techniques for peeking inside these networks.},
	author = {Mordvintsev, A. and Tyka, M. and Olah, C.},
	booktitle = {Google Research Blog},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	title = {{Inceptionism: Going deeper into neural networks, google research blog}},
	url = {https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html},
	year = {2015}
}

@article{Olah2017,
	abstract = {Deep convolutional neural networks (CNNs) have demonstrated impressive performance on visual object classification tasks. In addition, it is a useful model for predication of neuronal responses recorded in visual system. However, there is still no clear understanding of what CNNs learn in terms of visual neuronal circuits. Visualizing CNN's features to obtain possible connections to neuronscience underpinnings is not easy due to highly complex circuits from the retina to higher visual cortex. Here we address this issue by focusing on single retinal ganglion cells with a simple model and electrophysiological recordings from salamanders. By training CNNs with white noise images to predicate neural responses, we found that convolutional filters learned in the end are resembling to biological components of the retinal circuit. Features represented by these filters tile the space of conventional receptive field of retinal ganglion cells. These results suggest that CNN could be used to reveal structure components of neuronal circuits.},
	archivePrefix = {arXiv},
	arxivId = {1711.02837},
	author = {Olah, C. and Mordvintsev, A. and Schubert, L.},
	doi = {10.23915/distill.00007},
	eprint = {1711.02837},
	issn = {2476-0757},
	journal = {Distill},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	title = {{Feature Visualization}},
	url = {https://distill.pub/2017/feature-visualization/},
	year = {https://distill.pub/2017/feature-visualization/, 2017}
}

@article{Erhan2009,
	abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work},
	author = {Erhan, D. and Bengio, Y. and Courville, A. and Vincent, P.},
	file = {:home/juillermo/Desktop/visualization{\_}techreport.pdf:pdf},
	journal = {Bernoulli},
	keywords = {Feature visualization},
	mendeley-groups = {AML},
	mendeley-tags = {Feature visualization},
	title = {{Visualizing higher-layer features of a deep network}},
	year = {2009}
}

@inproceedings{10.1007/978-3-642-16001-1_30,
	abstract = {We investigate if interactions of longer range than typically considered in local protein secondary structure prediction methods can be captured in a simple machine learning framework to improve the prediction of {\$}\beta{\$} sheets. We use support vector machines and recursive feature elimination to show that the small signals available in long range interactions can indeed be exploited. The improvement is small but statistically significant on the benchmark datasets we used. We also show that feature selection within a long window and over amino acids at specific positions typically selects amino acids that are shown to be more relevant in the initiation and termination of {\$}\beta{\$}-sheet formation.},
	author = {Ni, Y. and Niranjan, M.},
	booktitle = {Pattern Recognition in Bioinformatics},
	isbn = {978-3-642-16001-1},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	pages = {349--357},
	publisher = {Springer Berlin Heidelberg},
	title = {{Exploiting Long-Range Dependencies in Protein $\beta$-Sheet Secondary Structure Prediction}},
	year = {2010}
}


@article{Zhou2018,
	abstract = {Protein secondary structure is the three dimensional form of local segments of proteins and its prediction is an important problem in protein tertiary structure prediction. Developing computational approaches for protein secondary structure prediction is becoming increasingly urgent. We present a novel deep learning based model, referred to as CNNH{\_}PSS, by using multi-scale CNN with highway. In CNNH{\_}PSS, any two neighbor convolutional layers have a highway to deliver information from current layer to the output of the next one to keep local contexts. As lower layers extract local context while higher layers extract long-range interdependencies, the highways between neighbor layers allow CNNH{\_}PSS to have ability to extract both local contexts and long-range interdependencies. We evaluate CNNH{\_}PSS on two commonly used datasets: CB6133 and CB513. CNNH{\_}PSS outperforms the multi-scale CNN without highway by at least 0.010 Q8 accuracy and also performs better than CNF, DeepCNF and SSpro8, which cannot extract long-range interdependencies, by at least 0.020 Q8 accuracy, demonstrating that both local contexts and long-range interdependencies are indeed useful for prediction. Furthermore, CNNH{\_}PSS also performs better than GSM and DCRNN which need extra complex model to extract long-range interdependencies. It demonstrates that CNNH{\_}PSS not only cost less computer resource, but also achieves better predicting performance. CNNH{\_}PSS have ability to extracts both local contexts and long-range interdependencies by combing multi-scale CNN and highway network. The evaluations on common datasets and comparisons with state-of-the-art methods indicate that CNNH{\_}PSS is an useful and efficient tool for protein secondary structure prediction.},
	author = {Zhou, J. and Wang, H. and Zhao, Z. and Xu, R. and Lu, Q.},
	doi = {10.1186/s12859-018-2067-8},
	file = {:home/juillermo/Desktop/document.pdf:pdf},
	issn = {14712105},
	journal = {BMC Bioinformatics},
	volumen = {19 (Suppl 4):60},
	pages = {99-109},
	keywords = {Convolutional neural network,Highway,Local context,Long-range interdependency,Protein secondary structure,Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	title = {{CNNH{\_}PSS: Protein 8-class secondary structure prediction by convolutional neural network with highway}},
	year = {2018}
}

@INPROCEEDINGS{8371925,
	author={C. Fang and Y. Shang and D. Xu},
	booktitle={2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)},
	title={A New Deep Neighbor Residual Network for Protein Secondary Structure Prediction},
	year={2017},
	volume={},
	number={},
	pages={66-71},
	keywords={Amino acids;Computer architecture;Neural networks;Protein sequence;Tools;Training;Protein secondary structure prediction;deep learning;deep neighbor residual network},
	doi={10.1109/ICTAI.2017.00022},
	ISSN={},
	month={Nov},}

@inproceedings{Johansen2017,
	abstract = {Deep learning has become the state-of-the-art method for predict-ing protein secondary structure from only its amino acid residues and sequence proole. Building upon these results, we propose to combine a bi-directional recurrent neural network (biRNN) with a conditional random meld (CRF), which we call the biRNN-CRF. The biRNN-CRF may be seen as an improved alternative to an auto-regressive uni-directional RNN where predictions are performed sequentially conditioning on the prediction in the previous time-step. The CRF is instead nearest neighbor-aware and models for the joint distribution of the labels for all time-steps. We condition the CRF on the output of biRNN, which learns a distributed represen-tation based on the entire sequence. The biRNN-CRF is therefore close to ideally suited for the secondary structure task because a high degree of cross-talk between neighboring elements can be ex-pected. We validate the model on several benchmark datasets. For example, on CB513, a model with 1.7 million parameters, achieves a Q8 accuracy of 69.4 for single model and 70.9 for ensemble, which to our knowledge is state-of-the-art. 1},
	author = {Johansen, A. R. and S{\o}nderby, C. K. and S{\o}nderby, S. K. and Winther, O.},
	booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics - ACM-BCB '17},
	doi = {10.1145/3107411.3107489},
	file = {:home/juillermo/Desktop/p73-johansen.pdf:pdf},
	isbn = {9781450347228},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	title = {{Deep Recurrent Conditional Random Field Network for Protein Secondary Prediction}},
	year = {2017}
}

@article{Wang2017,
	abstract = {The prediction of protein structures directly from amino acid sequences is one of the biggest challenges in computational biology. It can be divided into several independent sub-problems in which protein secondary structure (SS) prediction is fundamental. Many computational methods have been proposed for SS prediction problem. Few of them can model well both the sequence-structure mapping relationship between input protein features and SS, and the interaction relationship among residues which are both important for SS prediction. In this paper, we proposed a deep recurrent encoder–decoder networks called Secondary Structure Recurrent Encoder–Decoder Networks (SSREDNs) to solve this SS prediction problem. Deep architecture and recurrent structures are employed in the SSREDNs to model both the complex nonlinear mapping relationship between input protein features and SS, and the mutual interaction among continuous residues of the protein chain. A series of techniques are also used in this paper to refine the model's performance. The proposed model is applied to the open dataset CullPDB and CB513. Experimental results demonstrate that our method can improve both Q3 and Q8 accuracy compared with some public available methods. For Q8 prediction problem, it achieves 68.20{\%} and 73.1{\%} accuracy on CB513 and CullPDB dataset in fewer epochs better than the previous state-of-art method.},
	author = {Wang, Y. and Mao, H. and Yi, Z.},
	doi = {10.1016/j.knosys.2016.11.015},
	file = {:home/juillermo/Desktop/1-s2.0-S0950705116304713-main.pdf:pdf},
	issn = {09507051},
	journal = {Knowledge-Based Systems},
	volume = {118, 115-123},
	keywords = {Deep learning,Encoder–decoder networks,Recurrent neural networks,Secondary structure,Secondary structure prediction},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	title = {{Protein secondary structure prediction by using deep learning method}},
	year = {2017}
}


@article{Sønderby2014,
	abstract = {Prediction of protein secondary structure from the amino acid sequence is a classical bioinformatics problem. Common methods use feed forward neural networks or SVMs combined with a sliding window, as these models does not naturally handle sequential data. Recurrent neural networks are an generalization of the feed forward neural network that naturally handle sequential data. We use a bidirectional recurrent neural network with long short term memory cells for prediction of secondary structure and evaluate using the CB513 dataset. On the secondary structure 8-class problem we report better performance (0.674) than state of the art (0.664). Our model includes feed forward networks between the long short term memory cells, a path that can be further explored.},
	archivePrefix = {arXiv},
	arxivId = {1412.7828},
	author = {S{\o}nderby, S. K. and Winther, O.},
	eprint = {1412.7828},
	journal = {arXiv:1412.7828},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	title = {{Protein Secondary Structure Prediction with Long Short Term Memory Networks}},
	year = {2014}
}


@article{Magnan2014,
	abstract = {MOTIVATION Accurately predicting protein secondary structure and relative solvent accessibility is important for the study of protein evolution, structure and function and as a component of protein 3D structure prediction pipelines. Most predictors use a combination of machine learning and profiles, and thus must be retrained and assessed periodically as the number of available protein sequences and structures continues to grow. RESULTS We present newly trained modular versions of the SSpro and ACCpro predictors of secondary structure and relative solvent accessibility together with their multi-class variants SSpro8 and ACCpro20. We introduce a sharp distinction between the use of sequence similarity alone, typically in the form of sequence profiles at the input level, and the additional use of sequence-based structural similarity, which uses similarity to sequences in the Protein Data Bank to infer annotations at the output level, and study their relative contributions to modern predictors. Using sequence similarity alone, SSpro's accuracy is between 79 and 80{\%} (79{\%} for ACCpro) and no other predictor seems to exceed 82{\%}. However, when sequence-based structural similarity is added, the accuracy of SSpro rises to 92.9{\%} (90{\%} for ACCpro). Thus, by combining both approaches, these problems appear now to be essentially solved, as an accuracy of 100{\%} cannot be expected for several well-known reasons. These results point also to several open technical challenges, including (i) achieving on the order of ≥ 80{\%} accuracy, without using any similarity with known proteins and (ii) achieving on the order of ≥ 85{\%} accuracy, using sequence similarity alone. AVAILABILITY AND IMPLEMENTATION SSpro, SSpro8, ACCpro and ACCpro20 programs, data and web servers are available through the SCRATCH suite of protein structure predictors at http://scratch.proteomics.ics.uci.edu.},
	author = {Magnan, C. N. and Baldi, P.},
	doi = {10.1093/bioinformatics/btu352},
	isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
	issn = {14602059},
	journal = {Bioinformatics},
	pmid = {24860169},
	title = {{SSpro/ACCpro 5: Almost perfect prediction of protein secondary structure and relative solvent accessibility using profiles, machine learning and structural similarity}},
	year = {2014}
}


@article{Heffernan2017,
	abstract = {Motivation: The accuracy of predicting protein local and global structural properties such as sec-ondary structure and solvent accessible surface area has been stagnant for many years because of the challenge of accounting for non-local interactions between amino acid residues that are close in three-dimensional structural space but far from each other in their sequence positions. All exist-ing machine-learning techniques relied on a sliding window of 10–20 amino acid residues to cap-ture some 'short to intermediate' non-local interactions. Here, we employed Long Short-Term Memory (LSTM) Bidirectional Recurrent Neural Networks (BRNNs) which are capable of capturing long range interactions without using a window. Results: We showed that the application of LSTM-BRNN to the prediction of protein structural properties makes the most significant improvement for residues with the most long-range contacts (ji-jj {\textgreater}19) over a previous window-based, deep-learning method SPIDER2. Capturing long-range interactions allows the accuracy of three-state secondary structure prediction to reach 84{\%} and the correlation coefficient between predicted and actual solvent accessible surface areas to reach 0.80, plus a reduction of 5{\%}, 10{\%}, 5{\%} and 10{\%} in the mean absolute error for backbone /, w, h and s angles, respectively, from SPIDER2. More significantly, 27{\%} of 182724 40-residue models directly constructed from predicted Ca atom-based h and s have similar structures to their corresponding native structures (6{\AA} RMSD or less), which is 3{\%} better than models built by / and w angles. We expect the method to be useful for assisting protein structure and function prediction. Availability and implementation: The method is available as a SPIDER3 server and standalone package at http://sparks-lab.org.},
	author = {Heffernan, R. and Yang, Y. and Paliwal, K. and Zhou, Y.},
	doi = {10.1093/bioinformatics/btx218},
	file = {:home/juillermo/Desktop/btx218.pdf:pdf},
	issn = {14602059},
	journal = {Bioinformatics},
	keywords = {Computational Biology,LSTM,RNN,Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Computational Biology,LSTM,RNN,Secondary structure},
	number = {18},
	pages = {2842--2849},
	title = {{Capturing non-local interactions by long short-term memory bidirectional recurrent neural networks for improving prediction of protein secondary structure, backbone angles, contact numbers and solvent accessibility}},
	volume = {33},
	year = {2017}
}


@inproceedings{Hattori2017,
	abstract = {—One of the most important open problems in science is the protein secondary structures prediction from the protein sequence of amino acids. This work presents an application of Deep Recurrent Neural Network with Bidirectional Long Short-Term Memory (DBLSTM) cells to this problem. We compare the performance of the proposed approach with the state-of-the-art approaches. Despite the lower complexity of the proposed approach (i.e. Neural Network architecture with fewer neurons), results showed that the DBLSTM could achieve a satisfactory level of accuracy when compared with the state-of-the-art ap-proaches. We also studied the behavior of Gradient Optimizers applied to the DBLSTM. Furthermore, this paper concentrates on well-known quantitative analytical methods applied to evaluate the proposed approach.},
	annote = {Good technical info about DLSTM. Compares results to the Vanessa-previous paper.},
	author = {Hattori, L. T. and Benitez, C. M. V. and Lopes, H. S.},
	booktitle = {2017 IEEE Latin American Conference on Computational Intelligence (LA-CCI)},
	doi = {10.1109/LA-CCI.2017.8285678},
	file = {:home/juillermo/Desktop/08285678.pdf:pdf},
	isbn = {978-1-5386-3734-0},
	keywords = {Computational Biology,Deep Learning,LSTM,RNN,Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Computational Biology,Deep Learning,LSTM,RNN,Secondary structure},
	pages = {1--6},
	title = {{A deep bidirectional long short-term memory approach applied to the protein secondary structure prediction problem}},
	url = {http://ieeexplore.ieee.org/document/8285678/},
	year = {2017}
}


@article{Busia2017,
	abstract = {Recently developed deep learning techniques have significantly improved the accuracy of various speech and image recognition systems. In this paper we show how to adapt some of these techniques to create a novel chained convolutional architecture with next-step conditioning for improving performance on protein sequence prediction problems. We explore its value by demonstrating its ability to improve performance on eight-class secondary structure prediction. We first establish a state-of-the-art baseline by adapting recent advances in convolutional neural networks which were developed for vision tasks. This model achieves 70.0{\%} per amino acid accuracy on the CB513 benchmark dataset without use of standard performance-boosting techniques such as ensembling or multitask learning. We then improve upon this state-of-the-art result using a novel chained prediction approach which frames the secondary structure prediction as a next-step prediction problem. This sequential model achieves 70.3{\%} Q8 accuracy on CB513 with a single model; an ensemble of these models produces 71.4{\%} Q8 accuracy on the same test set, improving upon the previous overall state of the art for the eight-class secondary structure problem. Our models are implemented using TensorFlow, an open-source machine learning software library available at TensorFlow.org; we aim to release the code for these experiments as part of the TensorFlow repository.},
	archivePrefix = {arXiv},
	arxivId = {1702.03865},
	author = {Busia, A. and Jaitly, N.},
	eprint = {1702.03865},
	journal = {arXiv:1702.03865v1},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	title = {{Next-Step Conditioned Deep Convolutional Neural Networks Improve Protein Secondary Structure Prediction}},
	year = {2017}
}


@inproceedings{Li2016,
	abstract = {Protein secondary structure prediction is an important problem in bioinformatics. Inspired by the recent successes of deep neural networks, in this paper, we propose an end-to-end deep network that predicts protein secondary structures from integrated local and global contextual features. Our deep architecture leverages convolutional neural networks with different kernel sizes to extract multiscale local contextual features. In addition, considering long-range dependencies existing in amino acid sequences, we set up a bidirectional neural network consisting of gated recurrent unit to capture global contextual features. Furthermore, multi-task learning is utilized to predict secondary structure labels and amino-acid solvent accessibility simultaneously. Our proposed deep network demonstrates its effectiveness by achieving state-of-the-art performance, i.e., 69.7{\%} Q8 accuracy on the public benchmark CB513, 76.9{\%} Q8 accuracy on CASP10 and 73.1{\%} Q8 accuracy on CASP11. Our model and results are publicly available.},
	archivePrefix = {arXiv},
	arxivId = {1604.07176},
	author = {Li, Z. and Yu, Y.},
	journal = {arXiv:1604.07176v1},
	eprint = {1604.07176},
	isbn = {978-1-57735-770-4},
	issn = {10450823},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	title = {{Protein Secondary Structure Prediction Using Cascaded Convolutional and Recurrent Neural Networks}},
	year = {arXiv:1604.07176v1, 2016},
}


@article{Fang2017,
	abstract = {Motivation: Protein secondary structure prediction can provide important information for protein 3D structure prediction and protein functions. Deep learning, which has been successfully applied to various research fields such as image classification and voice recognition, provides a new opportunity to significantly improve the secondary structure prediction accuracy. Although several deep-learning methods have been developed for secondary structure prediction, there is room for improvement. MUFold-SS was developed to address these issues. Results: Here, a very deep neural network, the deep inception-inside-inception networks (Deep3I), is proposed for protein secondary structure prediction and a software tool was implemented using this network. This network takes two inputs: a protein sequence and a profile generated by PSI-BLAST. The output is the predicted eight states (Q8) or three states (Q3) of secondary structures. The proposed Deep3I not only achieves the state-of-the-art performance but also runs faster than other tools. Deep3I achieves Q3 82.8{\%} and Q8 71.1{\%} accuracies on the CB513 benchmark.},
	archivePrefix = {arXiv},
	arxivId = {1709.06165},
	author = {Fang, C. and Shang, Y. and Xu, D.},
	eprint = {1709.06165},
	journal = {arXiv:1790.06165},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fang, Shang, Xu - 2017 - MUFold-SS Protein Secondary Structure Prediction Using Deep Inception-Inside-Inception Networks.pdf:pdf},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	title = {{MUFold-SS: Protein Secondary Structure Prediction Using Deep Inception-Inside-Inception Networks}},
	url = {http://arxiv.org/abs/1709.06165},
	year = {2017}
}


@article{Jurtz2017,
	abstract = {Motivation: Deep neural network architectures such as convolutional and long short-term memory networks have become increasingly popular as machine learning tools during the recent years. The availability of greater computational resources, more data, new algorithms for training deep models and easy to use libraries for implementation and training of neural networks are the drivers of this development. The use of deep learning has been especially successful in image recognition; and the development of tools, applications and code examples are in most cases centered within this field rather than within biology. Results: Here, we aim to further the development of deep learning methods within biology by providing application examples and ready to apply and adapt code templates. Given such examples, we illustrate how architectures consisting of convolutional and long short-term memory neural networks can relatively easily be designed and trained to state-of-the-art performance on three biological sequence problems: prediction of subcellular localization, protein secondary structure and the binding of peptides to MHC Class II molecules. Availability: All implementations and datasets are available online to the scientific community at https://github.com/vanessajurtz/lasagne4bio. Supplementary information:Supplementary data are available at Bioinformatics online.},
	annote = {The key paper.},
	archivePrefix = {arXiv},
	arxivId = {103549},
	author = {Jurtz, V. I. and Johansen, A. R. and Nielsen, M. and {Almagro Armenteros}, J. J. and Nielsen, H. and S{\o}nderby, C. K. and Winther, O. and S{\o}nderby, S. K.},
	doi = {10.1093/bioinformatics/btx531},
	eprint = {103549},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jurtz et al. - 2017 - An introduction to deep learning on biological sequence data Examples and solutions.pdf:pdf},
	isbn = {1367-4811 (Electronic)},
	issn = {14602059},
	journal = {Bioinformatics},
	keywords = {Broad,CNN,Computational Biology,Deep learning,LSTM,RNN,Secondary structure,Sequencing},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Broad,CNN,Computational Biology,Deep learning,LSTM,RNN,Secondary structure,Sequencing},
	number = {22},
	pages = {3685--3690},
	pmid = {28961695},
	title = {{An introduction to deep learning on biological sequence data: Examples and solutions}},
	volume = {33},
	year = {2017}
}


@article{Wang2016,
	abstract = {Protein secondary structure (SS) prediction is important for studying protein structure and function. When only the sequence (profile) information is used as input feature, currently the best predictors can obtain {\~{}}80{\%} Q3 accuracy, which has not been improved in the past decade. Here we present DeepCNF (Deep Convolutional Neural Fields) for protein SS prediction. DeepCNF is a Deep Learning extension of Conditional Neural Fields (CNF), which is an integration of Conditional Random Fields (CRF) and shallow neural networks. DeepCNF can model not only complex sequence-structure relationship by a deep hierarchical architecture, but also interdependency between adjacent SS labels, so it is much more powerful than CNF. Experimental results show that DeepCNF can obtain {\~{}}84{\%} Q3 accuracy, {\~{}}85{\%} SOV score, and {\~{}}72{\%} Q8 accuracy, respectively, on the CASP and CAMEO test proteins, greatly outperforming currently popular predictors. As a general framework, DeepCNF can be used to predict other protein structure properties such as contact number, disorder regions, and solvent accessibility.},
	archivePrefix = {arXiv},
	arxivId = {1611.01503},
	author = {Wang, S. and Peng, J. and Ma, J. and Xu, J.},
	doi = {10.1038/srep18962},
	eprint = {1611.01503},
	file = {:home/juillermo/Desktop/srep18962.pdf:pdf},
	isbn = {2045-2322 (Electronic)$\backslash$r2045-2322 (Linking)},
	issn = {20452322},
	journal = {Scientific Reports},
	keywords = {CNN,Computational Biology,Deep learning,Secondary structure,Solving accessibility},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {CNN,Computational Biology,Deep learning,Secondary structure,Solving accessibility},
	pmid = {26752681},
	title = {{Protein Secondary Structure Prediction Using Deep Convolutional Neural Fields}},
	volume = {6},
	number = {18962},
	year = {2016}
}


@article{Lin2016,
	abstract = {Predicting protein properties such as solvent accessibility and secondary structure from its primary amino acid sequence is an important task in bioinformatics. Recently, a few deep learning models have surpassed the traditional window based multilayer perceptron. Taking inspiration from the image classification domain we propose a deep convolutional neural network architecture, MUST-CNN, to predict protein properties. This architecture uses a novel multilayer shift-and-stitch (MUST) technique to generate fully dense per-position predictions on protein sequences. Our model is significantly simpler than the state-of-the-art, yet achieves better results. By combining MUST and the efficient convolution operation, we can consider far more parameters while retaining very fast prediction speeds. We beat the state-of-the-art performance on two large protein property prediction datasets.},
	annote = {After the initial multitask model is trained, we take the top layers and each task-specific subclassifier and fine-tune the models by initializing their weights at the weights learned by the multitask model and training only on each specific task with 1/10 of the original learning rate. 
	
	PSI-BLAST generates a PSSM of size T × 20 for a T lengthed sequence, where a higher score represents a higher likelihood of the ith amino acid replacing the current one in other species. Generally, two amino acids that are interchangeable in the PSSM indicates that they are also interchangeable in the protein without sig- nificantly modifying the functionality of the protein.
	
	The class labels are H = alpha helix, B = residue in isolated beta bridge, E = extended strand, G = 3-helix, I = 5-helix, T = hydrogen bonded turn, S = bend, L = loop.
	
	ssp A collapsed version of the 8 class prediction task, since many protein secondary structure prediction algo- rithms use a 3 class approach instead of the 8-class ap- proach given in dssp. {\{}H, G{\}} → H =Helix, {\{}B, E{\}} → B =Beta sheet, and {\{}I, S, T, L{\}} → C =Coil},
	archivePrefix = {arXiv},
	arxivId = {1605.03004},
	author = {Lin, Z. and Lanchantin, J. and Qi, Y.},
	eprint = {1605.03004},
	file = {:home/juillermo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Lanchantin, Qi - 2016 - MUST-CNN A Multilayer Shift-and-Stitch Deep Convolutional Architecture for Sequence-based Protein Structure.pdf:pdf},
	keywords = {Secondary structure},
	mendeley-groups = {Deep Biology},
	mendeley-tags = {Secondary structure},
	month = {may},
	title = {{MUST-CNN: A Multilayer Shift-and-Stitch Deep Convolutional Architecture for Sequence-based Protein Structure Prediction}},
	url = {http://arxiv.org/abs/1605.03004},
	year = {2016}
}


